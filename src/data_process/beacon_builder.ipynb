{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, getsize, dirname\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shutil\n",
    "import warnings\n",
    "import hickle\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import feather\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "path = \"../../../../../../zion/OpenSNP/people\"\n",
    "meta = \"../../../../../../zion/OpenSNP/meta\"\n",
    "beacons = \"../../../../../zion/OpenSNP/beacon\"\n",
    "main_path = join(beacons, \"Main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(join(beacons, \"RMAF_3034.pickle\"), 'rb') as handle:\n",
    "    maf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 500, 1000, 1500, 2000, 2500, 3000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.arange(500,3001,500)\n",
    "\n",
    "for i in ind:\n",
    "    data = feather.read_dataframe(join(beacons, \"fBeacon_\"+str(i)+\".ftr\"))\n",
    "    data.set_index(\"rs_id\", inplace=True)\n",
    "    print(\"Read in: \", data.shape)\n",
    "    gc.collect()\n",
    "    data = maf.join(data, how=\"left\")\n",
    "    gc.collect()\n",
    "    del data[\"chr\"]\n",
    "    del data[\"count\"]\n",
    "    gc.collect()\n",
    "    data.fillna(\"NN\", inplace=True)\n",
    "    gc.collect()\n",
    "    print(\"Writing in: \", data.shape)\n",
    "    data.to_feather(join(beacons, \"RBeacon_\"+str(i)+\".ftr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(1000,3001,500)\n",
    "beacon = feather.read_dataframe(join(beacons, \"fBeacon_\"+str(500)+\".ftr\"))\n",
    "beacon.set_index(\"rs_id\", inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "for i in ind:\n",
    "    data = feather.read_dataframe(join(beacons, \"fBeacon_\"+str(i)+\".ftr\"))\n",
    "    data.set_index(\"rs_id\", inplace=True)\n",
    "    print(\"Read in: \", data.shape)\n",
    "    gc.collect()\n",
    "    beacon = pd.concat([beacon, data], axis=1)\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Writing in: \", data.shape)\n",
    "beacon.reset_index(inplace=True)\n",
    "beacon.to_feather(join(beacons, \"Beacon.ftr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon = feather.read_dataframe(join(beacons, \"Beacon.ftr\"))\n",
    "beacon.set_index(\"rs_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2338573, 2979)\n",
      "(2338573, 2)\n"
     ]
    }
   ],
   "source": [
    "print(beacon.shape)\n",
    "print(maf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf[\"major\"] = \"-\"\n",
    "maf[\"major_freq\"] = 0\n",
    "maf[\"minor\"] = \"-\"\n",
    "maf[\"minor_freq\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 47s, sys: 2min 2s, total: 23min 49s\n",
      "Wall time: 23min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def calculate(item):\n",
    "    line = ''.join(item).replace(\"N\",\"\")\n",
    "    return line\n",
    "res = np.apply_along_axis(calculate, 1, beacon.values)\n",
    "\n",
    "def foo(item):\n",
    "    return list(map(lambda c2: c2, item[0]))\n",
    "res = res.reshape(res.shape[0],1)\n",
    "res = [foo(res[i]) for i in range(len(res))]\n",
    "\n",
    "result = [list(Counter(e).items()) for e in res]\n",
    "\n",
    "result = np.array(result)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 1.03 s, total: 22.1 s\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maf[\"major\"] = [i[0][0] if i else \"-\" for i in result ]\n",
    "maf[\"major_freq\"] = [i[0][1] if i else 0 for i in result ]\n",
    "maf[\"minor\"] = [i[1][0] if len(i) > 1 else \"-\" for i in result ]\n",
    "maf[\"minor_freq\"] = [i[1][1] if len(i) > 1 else 0 for i in result ]\n",
    "\n",
    "t = maf[\"major_freq\"] + maf[\"minor_freq\"]\n",
    "maf[\"major_freq\"] = maf[\"major_freq\"] / t\n",
    "maf[\"minor_freq\"] = maf[\"minor_freq\"] / t\n",
    "maf = maf.fillna(0)\n",
    "maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Reference\n",
    "def getReference(maf):\n",
    "    greater = maf.loc[maf['major_freq'] > maf['minor_freq']]\n",
    "    equal   = maf.loc[maf['major_freq'] == maf['minor_freq']]\n",
    "    smaller = maf.loc[maf['major_freq'] < maf['minor_freq']]\n",
    "\n",
    "    greater[\"normal\"] = greater['major'] + \"\" + greater['major']\n",
    "    equal[\"normal\"] = equal['major'] + \"\" + equal['major'] #TODO\n",
    "    smaller[\"normal\"] = smaller['minor'] + \"\" + smaller['minor']\n",
    "    \n",
    "    x = pd.concat([greater,equal,smaller], axis=0)\n",
    "    x = x.sort_values(by=['rs_id'])\n",
    "    reference = x[\"normal\"].values\n",
    "    reference = np.expand_dims(reference, axis=1)\n",
    "    return reference\n",
    "\n",
    "reference = getReference(maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf.to_pickle(join(beacons, \"MAF.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(beacons, \"Reference.pickle\"), 'wb') as handle:\n",
    "    pickle.dump(reference, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findUserIndex(fileName):\n",
    "    fileName = fileName[4:]\n",
    "    return int(fileName.split(\"_\")[0])\n",
    "\n",
    "def findFileIndex(fileName):\n",
    "    return int(fileName.split(\"_\")[1][4:])\n",
    "\n",
    "def findSkipCount(fileName):\n",
    "    filePath = join(path, fileName)\n",
    "    with open(filePath, \"r\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if line[0] == \"#\" or line[0] == \" \":\n",
    "                i += 1\n",
    "            else:\n",
    "                if line[0:6] == \"rs-id\":\n",
    "                    i += 1\n",
    "                break\n",
    "        return i\n",
    "\n",
    "def readClean(data):\n",
    "    # Remove X,Y,MT chromosomes\n",
    "    no_x_y = np.logical_and(data[\"chromosome\"] != \"X\",data[\"chromosome\"] != \"Y\")\n",
    "    data = data[np.logical_and(no_x_y, data[\"chromosome\"] != \"MT\")]\n",
    "    data = data.fillna(\"NN\")\n",
    "    data[data == \"II\"] = \"NN\"\n",
    "    data[data == \"--\"] = \"NN\"\n",
    "    data[data == \"DD\"] = \"NN\"\n",
    "    data[data == \"DI\"] = \"NN\"\n",
    "    return data.iloc[np.where(data.iloc[:,[1]] != \"NN\")[0]]\n",
    "\n",
    "def readDf(file, rowSkip):\n",
    "    data = pd.read_csv(join(path, file), sep=\"\\t\", header=None, skiprows=rowSkip)\n",
    "    data.columns = ['rs_id', 'chromosome', 'position', 'allele']\n",
    "    del data['position']\n",
    "    data = data.set_index('rs_id')\n",
    "    data = data.rename(columns={\"allele\": findUserIndex(file)})\n",
    "    return data\n",
    "\n",
    "def readFileComplete(fileName):\n",
    "    rowSkip = findSkipCount(fileName)\n",
    "    beacon = readDf(fileName, rowSkip)\n",
    "    beacon = readClean(beacon)\n",
    "    return beacon\n",
    "\n",
    "def mergeClean(beacon):\n",
    "    beacon = beacon.loc[~beacon.index.duplicated(keep='first')]\n",
    "    beacon = beacon[pd.to_numeric(beacon['chr'], errors='coerce').notnull()]\n",
    "    beacon[\"chr\"] = pd.to_numeric(beacon[\"chr\"])\n",
    "    beacon = beacon.sort_values(by=['chr'])\n",
    "    beacon = beacon.fillna(\"NN\")\n",
    "    maf = beacon[['chr']]\n",
    "    beacon = beacon.drop(['chr'], axis=1)\n",
    "    t = np.where(np.sum(beacon.values != \"NN\", axis=1) > 1)[0]\n",
    "    beacon = beacon.iloc[t]\n",
    "    ind = []\n",
    "    for j in range(len(beacon.index)):\n",
    "        if beacon.index[j][0] == \"r\":\n",
    "            ind.append(j)\n",
    "    ind = np.array(ind)\n",
    "    beacon = beacon.iloc[ind]\n",
    "    beacon.columns = beacon.columns.astype(int)\n",
    "    return beacon, maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim except .23andme and small genome files\n",
    "files = np.array([f for f in listdir(path) if isfile(join(path, f))], dtype=str)\n",
    "types = []\n",
    "sizes = []\n",
    "for f in files:\n",
    "    types.append(f.split(\".\")[-2])\n",
    "    sizes.append(getsize(join(path,f)))\n",
    "types = np.array(types)\n",
    "sizes = np.array(sizes)\n",
    "Counter(types)\n",
    "ind = np.logical_and(types == \"23andme\", sizes > 15 * 1000000)\n",
    "files = files[ind]\n",
    "\n",
    "# Deal with multiple file people, select newest one\n",
    "user_filename = {}\n",
    "for f in files:\n",
    "    user_filename.setdefault(int(findUserIndex(f)),[]).append(f)\n",
    "multiple_files = {k:v for (k,v) in user_filename.items() if len(v) > 1}\n",
    "\n",
    "for m in multiple_files:\n",
    "    f_names = multiple_files.get(m)\n",
    "    selected = [findFileIndex(item) for item in f_names]\n",
    "    selected = selected.index(max(selected))\n",
    "    for i in range(len(f_names)):\n",
    "        if i != selected:\n",
    "            index = np.argwhere(files==f_names[i])\n",
    "            files = np.delete(files, index)\n",
    "\n",
    "user_filename = {}\n",
    "for f in files:\n",
    "    user_filename[int(findUserIndex(f))] = f\n",
    "user_ind = np.array(list(user_filename.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read phenotype file\n",
    "with open(join(beacons, \"OpenSNP_Phenotype.pickle\"), 'rb') as handle:\n",
    "    pheno = pickle.load(handle)\n",
    "print(pheno.shape)\n",
    "\n",
    "# Trim people have less phenotypes than threshold\n",
    "people_thres = 0\n",
    "x = np.sum(pheno != \"-\", axis=1)\n",
    "pheno = pheno.loc[x >= people_thres]\n",
    "pheno.shape\n",
    "\n",
    "files = [v for (k,v) in user_filename.items() if k in pheno.index.values]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Beacon\n",
    "print(\"Started main beacon build.\")\n",
    "beacon = readFileComplete(files[0])\n",
    "beacon = beacon.rename(columns={\"chromosome\": \"chr\"})\n",
    "i = 1\n",
    "while i < len(files):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        data = readFileComplete(files[i])\n",
    "        beacon = pd.merge(beacon, data, left_index=True, right_index=True, how='outer')\n",
    "        beacon[\"chr\"].fillna(beacon[\"chromosome\"], inplace=True)\n",
    "        beacon = beacon.drop(\"chromosome\", axis=1)\n",
    "    except:\n",
    "        print(\"File \" + files[i] + \" is skipped.\\n\")\n",
    "    end = time.time()\n",
    "    print(str(i) + \". step is completed in \" + str(end - start) + \" seconds.\")\n",
    "\n",
    "    if i % 100 == 0 or i == len(files) - 1:\n",
    "        print(\"Cleaning main beacon started.\")\n",
    "        beacon, maf = mergeClean(beacon)\n",
    "        print(\"Cleaned main beacon.\")\n",
    "        # SAVE\n",
    "        beacon.to_pickle(join(main_path, \"tBeacon_main_\"+str(i)+\".pickle\"))\n",
    "        maf.to_pickle(join(main_path, \"tMAF_main_\"+str(i)+\".pickle\"))\n",
    "        if i != len(files) - 1:\n",
    "            i+=1\n",
    "            print(\"\\n\" + str(i) + \" has started\")\n",
    "            beacon = readFileComplete(files[i])\n",
    "            beacon = beacon.rename(columns={\"chromosome\": \"chr\"})\n",
    "    i+=1\n",
    "print(\"Ended main beacon build.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([item for item, count in Counter(beacon.index.values).items() if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Join MAF's\n",
    "with open(join(main_path, \"tMAF_main_\"+str(3033)+\".pickle\"), 'rb') as handle:\n",
    "        maf = pickle.load(handle)\n",
    "ind = np.arange(100,3001,100)\n",
    "for i in ind:\n",
    "    with open(join(main_path, \"tMAF_main_\"+str(i)+\".pickle\"), 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    data = data.rename(columns={\"chr\": \"chromosome\"})\n",
    "    maf = pd.merge(maf, data, left_index=True, right_index=True, how='outer')\n",
    "    maf[\"chr\"] = maf['chr'].fillna(maf['chromosome'])\n",
    "    del maf[\"chromosome\"]\n",
    "    print(i, \" is completed.\")\n",
    "ind = []\n",
    "for j in range(len(maf.index)):\n",
    "    if maf.index[j][0] == \"r\":\n",
    "        ind.append(j)\n",
    "maf = maf.iloc[ind]\n",
    "ii = np.logical_and(maf[\"chr\"] != 0, maf[\"chr\"] != 25)\n",
    "ii = np.logical_and(maf[\"chr\"] != 26, ii)\n",
    "maf = maf[ii]\n",
    "maf.to_pickle(join(beacons, \"OMAF_3031.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Beacons\n",
    "j = 0\n",
    "block_size = 5\n",
    "ind = np.arange(500, 3001, 100)\n",
    "for i in ind:\n",
    "    if j % block_size == 0:\n",
    "        j += 1\n",
    "        with open(join(main_path, \"tBeacon_main_\" + str(i) + \".pickle\"), 'rb') as handle:\n",
    "            beacon = pickle.load(handle)\n",
    "        print(\" NEW START \", i, \" is started --> \", beacon.shape)\n",
    "        continue\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"\", i, \" is started --> \", beacon.shape)\n",
    "    with open(join(main_path, \"tBeacon_main_\" + str(i) + \".pickle\"), 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    cp1 = time.time()\n",
    "    print(\"Data is loaded in \", cp1 - start, \" seconds\")\n",
    "\n",
    "    beacon = pd.merge(beacon, data, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "    cp2 = time.time()\n",
    "    print(\"Merge is done in \", cp2 - cp1, \" seconds\")\n",
    "    j += 1\n",
    "    if j % block_size == 0:\n",
    "        print(\"SAVING MERGINGS \", i)\n",
    "        beacon.values[beacon.isnull().values] = \"NN\"\n",
    "        cp3 = time.time()\n",
    "        print(\"Filling NN is done in \", cp3 - cp2, \" seconds\")\n",
    "        #beacon.to_pickle(join(beacons, \"kBeacon_\" + str(i) + \".pickle\"))\n",
    "        beacon.to_parquet(join(beacons, \"kBeacon_\" + str(i) + \".parquet\"), engine='fastparquet')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "beacon_1400 = pd.read_parquet(join(beacons, \"kBeacon_\"+str(1400)+\".parquet\"), engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "beacon_1900 = pd.read_parquet(join(beacons, \"kBeacon_\"+str(1900)+\".parquet\"), engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon = pd.merge(beacon, data, left_index=True, right_index=True, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon.values[beacon.isnull().values] = \"NN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon.to_parquet(join(beacons, \"kBeacon_\" + str(i) + \".parquet\"), engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beacon_1400.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a file\n",
    "beacon.to_pickle(join(beacons, \"Beacon_3031.pickle\"))\n",
    "hickle.dump(beacon, join(beacons, \"Beacon_3031.pickle\"), mode='w')\n",
    "maf.to_pickle(join(beacons, \"OpenSNP_MAF_3031.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios2 = np.sum(beacon == \"NN\", axis=1)\n",
    "ratios2 = (ratios2 / (beacon.shape[1]/100)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.hist(100-ratios, bins=np.arange(0,105,5), alpha=0.5, label='OpenSNP')\n",
    "plt.title(\"OpenSNP\")\n",
    "plt.xticks(np.arange(0,105,5))\n",
    "plt.xlabel(\"Percentage of Sequenced People\")\n",
    "plt.ylabel(\"Number of SNP's\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "_ = plt.hist(100-ratios2, bins=np.arange(0,105,5), alpha=0.5, label='OpenSNP')\n",
    "plt.title(\"OpenSNP\")\n",
    "plt.xticks(np.arange(0,105,5))\n",
    "plt.xlabel(\"Percentage of Sequenced People\")\n",
    "plt.ylabel(\"Number of SNP's\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "ind = np.arange(100,3001,100)\n",
    "for i in ind:\n",
    "    with open(join(main_path, \"tBeacon_main_\"+str(i)+\".pickle\"), 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    print(\"Loaded data.\")\n",
    "    data = maf.join(data, how='left')\n",
    "    data.fillna(\"NN\", inplace=True)\n",
    "    print(\"NN filled.\")\n",
    "    del data[\"chr\"]\n",
    "    print(\"Dropped chr.\")\n",
    "    data.to_pickle(join(beacons, \"aBeacon_main\"+str(i)+\".pickle\"))\n",
    "    print(i, \" is dumped, DONE.\\n\")\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(join(beacons, \"MAF_3034.pickle\"), 'rb') as handle:\n",
    "    maf = pickle.load(handle)\n",
    "print(maf.shape)\n",
    "\n",
    "maf[\"count\"] = 0\n",
    "\n",
    "%%time\n",
    "data2 = feather.read_dataframe(join(beacons, \"fBeacon_3000.ftr\"))\n",
    "data2.set_index(\"rs_id\", inplace=True)\n",
    "print(data2.shape)\n",
    "\n",
    "sums = np.sum(data2.values != \"NN\", axis=1)\n",
    "temp = np.where(maf.index.isin(data2.index))\n",
    "col = np.zeros(maf.shape[0])\n",
    "col[temp] = sums\n",
    "maf[\"current\"] = col\n",
    "maf[\"count\"] += maf[\"current\"]\n",
    "gc.collect()\n",
    "\n",
    "del maf[\"current\"]\n",
    "with open(join(beacons, \"MAF_3034.pickle\"), 'wb') as handle:\n",
    "    pickle.dump(maf, handle)\n",
    "\n",
    "maf2 = maf[maf[\"count\"] >= 8]\n",
    "\n",
    "with open(join(beacons, \"RMAF_3034.pickle\"), 'wb') as handle:\n",
    "    pickle.dump(maf2, handle)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
