{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import timeit\n",
    "import itertools\n",
    "import warnings\n",
    "import pickle\n",
    "import feather\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join, isfile\n",
    "from collections import Counter\n",
    "from xgboost import XGBClassifier\n",
    "from fcmeans import FCM\n",
    "import scipy.stats as stats\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error, classification_report, mutual_info_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier,BalancedBaggingClassifier, EasyEnsembleClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainPath = \"../../data\"\n",
    "beacons = join(mainPath, \"beacon\")\n",
    "testSets = join(\"\", \"test_sets\")\n",
    "models = join(mainPath, \"models\")\n",
    "ceuPath = join(beacons, \"CEU\")\n",
    "opensnpPath = join(beacons, \"OpenSNP\")\n",
    "inferencePath = join(\"\", \"inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Load Beacon, MAF, Reference and other cached variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ 'EyeColor','HairType','HairColor','TanAbility','Asthma','LactoseIntolerance',#'BloodType',\n",
    "             'EarWax','Freckling','TongueRoller','RingFinger','Intolerance','WidowPeak','ADHD','Acrophobia',\n",
    "             'FingerHair','Myopia','IrritableBowel','IndexLongerBig','Photoptarmis','Migraine','RhProtein']\n",
    "with open(join(opensnpPath, \"OpenSNP_Phenotype.pickle\"), 'rb') as handle:\n",
    "    pheno = pickle.load(handle)\n",
    "pheno = pheno[features]\n",
    "pheno[pheno==\"Auburn\"] = \"Blonde\"\n",
    "pheno[pheno==\"Black\"] = \"Brown\"\n",
    "\n",
    "with open(join(opensnpPath, \"MAF.pickle\"), 'rb') as handle:\n",
    "    maf = pickle.load(handle)\n",
    "\n",
    "with open(join(opensnpPath, \"Reference.pickle\"), 'rb') as handle:\n",
    "    reference = pickle.load(handle)\n",
    "reference = reference.values\n",
    "\n",
    "with open(join(opensnpPath, \"Beacon.pickle\"), 'rb') as handle:\n",
    "    beacon = pickle.load(handle)\n",
    "\n",
    "with open(join(opensnpPath, \"BinaryBeacon.pickle\"), 'rb') as handle:\n",
    "    binary = pickle.load(handle)\n",
    "    \n",
    "with open(join(opensnpPath, \"TernaryBeacon.pickle\"), 'rb') as handle:\n",
    "    ternary = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constrainted Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno5People = pheno.iloc[np.where(np.sum(pheno != \"-\", axis = 1) >=10)[0]].index\n",
    "pheno5People = pheno5People.map(str)\n",
    "pheno5People = np.where(beacon.columns.isin(pheno5People))[0]\n",
    "\n",
    "pheno1People = pheno.iloc[np.where(np.sum(pheno != \"-\", axis = 1) >= 1)[0]].index\n",
    "pheno1People = pheno1People.map(str)\n",
    "pheno1People = np.where(beacon.columns.isin(pheno1People))[0]\n",
    "\n",
    "phenoAllPeople = np.arange(beacon.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 668, 0: 536, 2: 116})\n",
      "[[517.16 18.28 0.50 -2.28]\n",
      " [-343.34 -188.36 2.14 1.20]\n",
      " [-412.45 1000.26 -14.64 3.62]]\n",
      "(536,) (668,) (116,)\n",
      "(94,) (60,) (5,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=4)\n",
    "tr1 = pca.fit_transform(ternary[:, pheno1People].T)\n",
    "plt.scatter(tr1[:, 0], tr1[:, 1], alpha=0.4)\n",
    "\n",
    "kmeans = KMeans(3)\n",
    "y_kmeans = kmeans.fit_predict(tr1)\n",
    "plt.scatter(tr1[:, 0], tr1[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "print(Counter(y_kmeans))\n",
    "print(centers)\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# Get indices of homogeneous groups\n",
    "g1 = pheno1People[np.where(y_kmeans == 0)[0]]\n",
    "g2 = pheno1People[np.where(y_kmeans == 1)[0]]\n",
    "g3 = pheno1People[np.where(y_kmeans == 2)[0]]\n",
    "print(g1.shape, g2.shape, g3.shape)\n",
    "\n",
    "g1a = np.intersect1d(g1, pheno5People)\n",
    "g2a = np.intersect1d(g2, pheno5People)\n",
    "g3a = np.intersect1d(g3, pheno5People)\n",
    "print(g1a.shape, g2a.shape, g3a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.2: Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beacon operations\n",
    "def queryBeacon(beacon_people):\n",
    "    return binary[:, beacon_people].any(axis=1)\n",
    "\n",
    "def getMutationAt(index):\n",
    "    temp = maf.iloc[index]\n",
    "    if temp[\"minor_freq\"] == temp[\"maf\"]:\n",
    "        return temp[\"minor\"] + temp[\"major\"] \n",
    "    else:\n",
    "        return temp[\"major\"] + temp[\"minor\"] \n",
    "\n",
    "def div(n, d):\n",
    "    return n / d if d else 0\n",
    "\n",
    "def rpaCalculate(tp,fp,tn,fn):\n",
    "    recall = div(tp,(tp+fn)) \n",
    "    precision = div(tp,(tp+fp))\n",
    "    accuracy = div((tp+tn),(tp+fp+tn+fn))\n",
    "    return recall, precision, accuracy\n",
    "\n",
    "def getTrainingData(phenotype, pos, test_people):\n",
    "    # Find indices of people who has the specified feature\n",
    "    feature_label = pheno[pheno[phenotype] != \"-\"][phenotype]\n",
    "    existing = beacon.columns.isin(feature_label.index.values)\n",
    "    existing[test_people] = False \n",
    "    \n",
    "    # Get training data\n",
    "    X = binary[pos][:, existing].T\n",
    "    Y = feature_label[beacon.columns[existing]].values\n",
    "    return X, Y\n",
    "\n",
    "# Performance method\n",
    "def performance(person, reconstruction, eval_pos, reference):\n",
    "    ind = np.logical_and(person[eval_pos] != np.squeeze(reference)[eval_pos], person[eval_pos] != \"NN\")\n",
    "    tp = np.sum(reconstruction[eval_pos][ind] != np.squeeze(reference)[eval_pos][ind])\n",
    "    fn = np.sum(ind) - tp\n",
    "    fp = np.sum(reconstruction[eval_pos][~ind] != np.squeeze(reference)[eval_pos][~ind])\n",
    "    tn = np.sum(~ind) - fp\n",
    "\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def performance_f(test_people, reconstructed, add_count, cluster_count, eval_pos):\n",
    "    total_values = np.zeros((4))\n",
    "    best_matches = []\n",
    "    # For all people in victim set\n",
    "    for i in range(add_count):\n",
    "        all_combinations = np.zeros((4, cluster_count))\n",
    "        rpa = np.zeros((3, cluster_count))\n",
    "        # For each cluster obtained\n",
    "        for j in range(cluster_count):\n",
    "            all_combinations[:, j] = performance(test_people[i], reconstructed[j], eval_pos, reference)\n",
    "            rpa[:, j] = rpaCalculate(*all_combinations[:, j])\n",
    "        ind = np.argmax(rpa[0,:]*rpa[1,:])       #Best-match index\n",
    "        best_matches.append(ind)\n",
    "        total_values += all_combinations[:, ind] #Add total tp-fp-tn-fn\n",
    "    recall, precision, accuracy = rpaCalculate(*total_values)\n",
    "    print(\"Recall_Micro_Avg    =\", round(recall, 2),\"\\nPrecision_Micro_Avg =\", round(precision, 2))\n",
    "    return (precision,recall,accuracy), total_values, best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Choose random people and send query to Beacon to obtain No-Yes answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoYes(add_count, beacon_size):\n",
    "\n",
    "    # Take people for added group\n",
    "    added_people = pheno5People.copy()\n",
    "    random.shuffle(added_people)\n",
    "    added_people = added_people[:add_count]\n",
    "    \n",
    "    # Take people for beacon\n",
    "    beacon_people = np.setdiff1d(phenoAllPeople, added_people)\n",
    "    random.shuffle(beacon_people)\n",
    "    beacon_people = beacon_people[:beacon_size]\n",
    "\n",
    "    # Query Beacon initially\n",
    "    before = queryBeacon(beacon_people)\n",
    "    # Add people\n",
    "    updated_beacon = np.concatenate([added_people,beacon_people])\n",
    "    # Query Beacon again\n",
    "    after = queryBeacon(updated_beacon)\n",
    "    # Find No-Yes SNPs' indices\n",
    "    no_yes_indices = np.where(np.logical_and(before==False, after==True))[0]\n",
    "    yes_yes_indices = np.where(np.logical_and(before==True, after==True))[0]\n",
    "    print(\"Number of No-Yes SNP's : \", len(no_yes_indices))\n",
    "    \n",
    "    return yes_yes_indices, no_yes_indices, added_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f44f80a97b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.arange(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoYes2(add_count, beacon_size):\n",
    "\n",
    "    # Take people for added group\n",
    "    random.shuffle(g1a)\n",
    "    random.shuffle(g2a)\n",
    "    random.shuffle(g3a)\n",
    "    if add_count >= 5:\n",
    "        added_people = np.concatenate([g1a[:(2 * add_count // 5)], g2a[:(2 * add_count // 5)], g3a[:(add_count // 5)]])\n",
    "    elif add_count == 3:\n",
    "        added_people = np.concatenate([g1a[:(add_count // 3)], g2a[:(add_count // 3)], g3a[:(add_count // 3)]])\n",
    "    elif add_count == 2:\n",
    "        added_people = np.concatenate([g1a[:(add_count // 2)], g2a[:(add_count // 2)]])\n",
    "\n",
    "    # Take people for beacon\n",
    "    g1_ = np.setdiff1d(g1, g1a)\n",
    "    random.shuffle(g1_)\n",
    "    g2_ = np.setdiff1d(g2, g2a)\n",
    "    random.shuffle(g2_)\n",
    "    g3_ = np.setdiff1d(g3, g3a)\n",
    "    random.shuffle(g3_)\n",
    "    if add_count >= 5:\n",
    "        curBeacon = np.concatenate([g1_[:(2 * beacon_size // 5)], g2_[:(2 * beacon_size // 5)], g3_[:(beacon_size // 5)]])\n",
    "    elif add_count == 3:\n",
    "        curBeacon = np.concatenate([g1_[:(beacon_size // 3)], g2_[:(beacon_size // 3)], g3_[:(beacon_size // 3)]])\n",
    "    elif add_count == 2:\n",
    "        curBeacon = np.concatenate([g1_[:(beacon_size // 2)], g2_[:(beacon_size // 2)]])\n",
    "\n",
    "    # Query Beacon initially\n",
    "    before = queryBeacon(curBeacon)\n",
    "    # Add people\n",
    "    updatedBeacon = np.concatenate([added_people, curBeacon])\n",
    "    # Query Beacon again\n",
    "    after = queryBeacon(updatedBeacon)\n",
    "    # Find No-Yes SNPs' indices\n",
    "    no_yes_indices = np.where(np.logical_and(before == False, after == True))[0]\n",
    "    yes_yes_indices = np.where(np.logical_and(before == True, after == True))[0]\n",
    "    print(\"Number of No-Yes SNP's : \", len(no_yes_indices))\n",
    "\n",
    "    return yes_yes_indices, no_yes_indices, added_people, curBeacon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: Correlation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def builtSNPNetwork(no_yes_indices, model_ind, reference):\n",
    "    model = ternary[no_yes_indices][:, model_ind].astype(float)\n",
    "    model[model==-1] = np.nan\n",
    "    x = pairwise_distances(model, metric = \"sokalmichener\", n_jobs=-1)\n",
    "    x = 1-np.nan_to_num(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectralClustering(no_yes_indices, add_count, x, reference, cluster_count=None):\n",
    "    if not cluster_count:\n",
    "        cluster_count = add_count\n",
    "    sc = SpectralClustering(cluster_count, affinity='precomputed', n_init=100, n_jobs=-1)\n",
    "    sc.fit(np.array(x))\n",
    "    bins = []\n",
    "    for i in range(cluster_count):\n",
    "        temp = []\n",
    "        for element in np.where(sc.labels_==i)[0]:\n",
    "            temp.append(no_yes_indices[element])\n",
    "        #print(\"Bin \" + str(i) + \" has \" + str(len(temp)) + \" SNP's\")\n",
    "        bins.append(temp)\n",
    "    reconstructed = np.array([reference.T[0] for i in range(cluster_count)])\n",
    "    for i in range(cluster_count):\n",
    "        for j in bins[i]:\n",
    "            reconstructed[i][j] = getMutationAt(j)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuzzy Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzyClustering(no_yes_indices, add_count, x, reference):\n",
    "    fcm = FCM(n_clusters=add_count)\n",
    "    fcm.fit(x)\n",
    "    soft_clusters = fcm.u \n",
    "    bins = [[] for i in range(add_count)]\n",
    "    for i in range(len(soft_clusters)):\n",
    "        maxPos = np.argmax(soft_clusters[i])\n",
    "        if  soft_clusters[i][maxPos] <= 0.5:\n",
    "            for j in np.where(soft_clusters[i] > (soft_clusters[i][maxPos]*2/3))[0]:\n",
    "                bins[j].append(no_yes_indices[i])\n",
    "        else:\n",
    "            bins[maxPos].append(no_yes_indices[i])\n",
    "    reconstructed = np.array([reference.T[0] for i in range(cluster_count)])\n",
    "    for i in range(cluster_count):\n",
    "        for j in bins[i]:\n",
    "            reconstructed[i][j] = getMutationAt(j)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E X P E R I M E N T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of No-Yes SNP's :  11473\n"
     ]
    }
   ],
   "source": [
    "add_count = 5\n",
    "cluster_count = 5\n",
    "beacon_size = 50\n",
    "yes_yes_ind, no_yes_ind, added_people, beacon_people     = getNoYes2(add_count, beacon_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all the people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall_Micro_Avg    = 0.91 \n",
      "Precision_Micro_Avg = 0.83\n",
      "CPU times: user 3min 15s, sys: 8.67 s, total: 3min 24s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_ind = pheno1People\n",
    "correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "(precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using other people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall_Micro_Avg    = 0.91 \n",
      "Precision_Micro_Avg = 0.83\n",
      "CPU times: user 3min 6s, sys: 6.49 s, total: 3min 13s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_ind = np.setdiff1d(pheno1People, added_people)\n",
    "model_ind = np.setdiff1d(model_ind, beacon_people)\n",
    "\n",
    "correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "(precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_count = 10\n",
    "cluster_count = 10\n",
    "beacon_size = 100\n",
    "yes_yes_ind, no_yes_ind, added_people     = getNoYes(add_count, beacon_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "correlations                              = builtSNPNetwork(no_yes_ind, pheno1People, reference)\n",
    "reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "(precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Brute Force Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiments = [(2, 20, 0.9), (3, 30, 0.8), (5, 50, 0.8), (10, 100, 0.8), (20, 100, 0.65), (30, 100, 0.6), (40, 100, 0.55)]\n",
    "\n",
    "for e in experiments:\n",
    "    add_count = e[0]\n",
    "    cluster_count = add_count\n",
    "    beacon_size = e[1]\n",
    "    target = e[2]\n",
    "    test_sets = []\n",
    "    for i in range(10):\n",
    "        precision, recall = 0, 0\n",
    "        while precision + recall < target * 2:\n",
    "            yes_yes_ind, no_yes_ind, added_people     = getNoYes(add_count, beacon_size)\n",
    "            correlations                              = builtSNPNetwork(no_yes_ind, pheno5People, reference)\n",
    "            reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "            (precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)\n",
    "        gs = [yes_yes_ind, no_yes_ind, added_people]\n",
    "        test_sets.append(gs)\n",
    "    filename = str(e[0]) + \"_testset.pkl\" \n",
    "    with open(join(beacons, filename), 'wb') as f:\n",
    "        pickle.dump(test_sets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotype Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(models, x_test, y_test, add_count, cluster_count):\n",
    "    # Predict\n",
    "    results = []\n",
    "    for i in models:\n",
    "        results.append(i[1].predict_proba(x_test))\n",
    "    labels = [i[0] for i in models]\n",
    "\n",
    "    top3, top1 = 0, 0\n",
    "    # For each person\n",
    "    for i in range(add_count):\n",
    "        test_person = y_test[labels].iloc[i]\n",
    "        available_phenotypes = np.where(test_person != \"-\")[0]\n",
    "\n",
    "        # For each reconstructed genome\n",
    "        probs = np.zeros((cluster_count))\n",
    "        for j in range(cluster_count):\n",
    "            # For each available phenotype\n",
    "            for k in available_phenotypes:\n",
    "                target_label_ind = np.where(models[k][1].classes_ == test_person[k])[0]\n",
    "                probs[j] += results[k][j][target_label_ind]# * models[k][2]\n",
    "\n",
    "        # Top k\n",
    "        matched_ind = np.argsort(probs)[-3:]\n",
    "        print(probs, \"\\n\", matched_ind, \"--\", matches[i], \"\\n\")\n",
    "        if matches[i] in matched_ind:\n",
    "            top3 += 1\n",
    "        if matches[i] == matched_ind[-1]:\n",
    "            top1 += 1\n",
    "    print(\"Top-1 Accuracy= \", top1 / add_count, \"\\tTop-3 Accuracy= \", top3 / add_count)\n",
    "    return top1 / add_count, top3 / add_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimators= [100]  # n_estimators\n",
    "Depths    = [3,4,6,None] # max_depth (None olabilir)\n",
    "MinSample = [4,8,10,12]  # min_samples_leaf\n",
    "MaxFeatures = [\"auto\", 1/2, 1/4]  # min_samples_leaf\n",
    "\n",
    "#Impurity  = [0]  # min_impurity_decrease\n",
    "Criterion = [\"gini\"]                  # criterion\n",
    "parameters = {\"max_depth\":Depths,\"min_samples_leaf\":MinSample,\"criterion\":Criterion,\"n_estimators\":Estimators,\"max_features\":MaxFeatures}\n",
    "              #\"min_impurity_decrease\":Impurity,\"min_samples_split\":MinSplit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_snps, test_people):\n",
    "    models = []\n",
    "    count = 1\n",
    "    for feature in features:\n",
    "        X, Y = getTrainingData(phenotype=feature, pos=train_snps, test_people=test_people)\n",
    "        print(\"\\n\",count, \".\", feature, \"\\tlabels=\", np.unique(Y))\n",
    "        \n",
    "        # Upsampling\n",
    "        X, Y = SMOTE().fit_sample(X, Y)\n",
    "        # Train the model\n",
    "        rf = RandomForestClassifier(class_weight='balanced_subsample',oob_score=True,n_jobs=-1)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=20)\n",
    "        model = GridSearchCV(cv=cv, estimator=rf, scoring='f1_macro', param_grid=parameters,verbose=0,n_jobs=-1)\n",
    "        result = model.fit(X, Y)\n",
    "\n",
    "        print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "        best_model = result.best_estimator_\n",
    "        best_score = (result.best_score_ + best_model.oob_score_) / 2\n",
    "        #best_model.fit(X, Y)\n",
    "        \n",
    "        if best_score > 1.2 / len(np.unique(Y)):\n",
    "            count += 1\n",
    "            print(\"Train:\", round(best_model.score(X, Y), 2), \" | Validation:\", round(best_score,2))\n",
    "            models.append((feature, model, best_score))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiments = [(2,20, 0.9),(3,30, 0.8),(5,50, 0.8),(10,100, 0.8),(20,100, 0.65)]\n",
    "\n",
    "e = experiments[0]\n",
    "add_count = e[0]\n",
    "beacon_size = e[1]\n",
    "\n",
    "with open(join(testSets, str(add_count) + \"_testset.pkl\"), 'rb') as f:\n",
    "    test_sets = pickle.load(f)\n",
    "\n",
    "top1s, top3s = [], []\n",
    "for i in range(10):\n",
    "    yes_yes_ind, no_yes_ind, added_people = test_sets[i]\n",
    "    model_ind = np.setdiff1d(pheno1People, added_people)\n",
    "     \n",
    "    # Genome Reconstruction    \n",
    "    correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "    reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "    (precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,add_count,no_yes_ind)\n",
    "    \n",
    "    # Phenotype Prediction\n",
    "    models = train_models(train_snps=no_yes_ind, test_people=added_people)\n",
    "    \n",
    "    # Test Data\n",
    "    x_test = (reconstructed_spectral[:, no_yes_ind] != reference[no_yes_ind].T).astype(np.int8)\n",
    "    y_test = pheno.loc[beacon.columns[added_people]]\n",
    "    \n",
    "    # Performance\n",
    "    top1, top3 = evaluate_ensemble(models, x_test, y_test, add_count, add_count)\n",
    "    top1s.append(top1)\n",
    "    top3s.append(top3)\n",
    "print(\"Top-1= \", np.mean(top1s), \"\\tTop-3= \", np.mean(top3s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [(2,20, 0.9),(3,30, 0.8),(5,50, 0.8),(10,100, 0.8),(20,100, 0.65)]\n",
    "res = []\n",
    "for e in experiments:\n",
    "    add_count = e[0]\n",
    "    beacon_size = e[1]\n",
    "    with open(join(testSets, str(add_count) + \"_testset.pkl\"), 'rb') as f:\n",
    "        test_sets = pickle.load(f)\n",
    "    top1s = []\n",
    "    top3s = []\n",
    "    for i in range(10):\n",
    "        yes_yes_ind, no_yes_ind, added_people = test_sets[i]\n",
    "        model_ind = np.setdiff1d(pheno1People, added_people)\n",
    "\n",
    "        # Genome Reconstruction    \n",
    "        correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "        reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "        (precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,add_count,no_yes_ind)\n",
    "\n",
    "        # Phenotype Prediction\n",
    "        models = train_models(train_snps=no_yes_ind, test_people=added_people)\n",
    "\n",
    "        # Test Data\n",
    "        x_test = (reconstructed_spectral[:, no_yes_ind] != reference[no_yes_ind].T).astype(np.int8)\n",
    "        y_test = pheno.loc[beacon.columns[added_people]]\n",
    "\n",
    "        # Performance\n",
    "        top1, top3 = evaluate_ensemble(models, x_test, y_test, add_count, add_count)\n",
    "        top1s.append(top1)\n",
    "        top3s.append(top3)\n",
    "    print(\"Top-1= \", np.mean(top1s), \"\\tTop-3= \", np.mean(top3s))\n",
    "    res.append((top1s,top3s))\n",
    "    with open(join(beacons, str(add_count) + \".pkl\"), 'wb') as f:\n",
    "        pickle.dump((top1s,top3s), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1s = []\n",
    "top3s = []\n",
    "for i in [2,5,10,20]:\n",
    "    with open(join(beacons, str(i)+\".pkl\"), 'rb') as f:\n",
    "        t = pickle.load(f)\n",
    "    top1s.append(np.mean(t[0]))\n",
    "    top3s.append(np.mean(t[1]))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Number of Added People\")\n",
    "plt.yticks(np.arange(0,1.06,0.05))\n",
    "plt.ylim(0,1.04)\n",
    "my_xticks = ['2','5','10','20']\n",
    "plt.xticks(np.array([0,1,2,3]), my_xticks)\n",
    "plt.plot(top1s, label=\"Top-1\")\n",
    "plt.plot(top3s, label=\"Top-3\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "#plt.savefig('phenotype.jpg', format='jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "STARTED\n",
      "Number of No-Yes SNP's :  2109\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.94\n",
      "Number of No-Yes SNP's :  1510\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3040\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.97\n",
      "Number of No-Yes SNP's :  9811\n",
      "Recall_Micro_Avg    = 0.98 \n",
      "Precision_Micro_Avg = 0.98\n",
      "Number of No-Yes SNP's :  1535\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9537\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1664\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  15683\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1353\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  12316\n",
      "Recall_Micro_Avg    = 0.88 \n",
      "Precision_Micro_Avg = 0.51\n",
      "Number of No-Yes SNP's :  1205\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2368\n",
      "Recall_Micro_Avg    = 0.9 \n",
      "Precision_Micro_Avg = 0.52\n",
      "Number of No-Yes SNP's :  2250\n",
      "Recall_Micro_Avg    = 0.93 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  1725\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  11198\n",
      "Recall_Micro_Avg    = 0.85 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  4359\n",
      "Recall_Micro_Avg    = 0.76 \n",
      "Precision_Micro_Avg = 0.54\n",
      "Number of No-Yes SNP's :  2486\n",
      "Recall_Micro_Avg    = 0.91 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  2926\n",
      "Recall_Micro_Avg    = 0.95 \n",
      "Precision_Micro_Avg = 0.95\n",
      "Number of No-Yes SNP's :  4442\n",
      "Recall_Micro_Avg    = 0.87 \n",
      "Precision_Micro_Avg = 0.54\n",
      "Number of No-Yes SNP's :  2228\n",
      "Recall_Micro_Avg    = 0.92 \n",
      "Precision_Micro_Avg = 0.92\n",
      "Number of No-Yes SNP's :  3596\n",
      "Recall_Micro_Avg    = 0.97 \n",
      "Precision_Micro_Avg = 0.98\n",
      "Number of No-Yes SNP's :  10803\n",
      "Recall_Micro_Avg    = 0.99 \n",
      "Precision_Micro_Avg = 0.99\n",
      "Number of No-Yes SNP's :  2962\n",
      "Recall_Micro_Avg    = 0.91 \n",
      "Precision_Micro_Avg = 0.52\n",
      "Number of No-Yes SNP's :  2392\n",
      "Recall_Micro_Avg    = 0.93 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  2217\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.94\n",
      "Number of No-Yes SNP's :  1466\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3106\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9511\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1153\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  12675\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.94\n",
      "Number of No-Yes SNP's :  3476\n",
      "Recall_Micro_Avg    = 0.97 \n",
      "Precision_Micro_Avg = 0.98\n",
      "Number of No-Yes SNP's :  3093\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1150\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1083\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6718\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.94\n",
      "Number of No-Yes SNP's :  2387\n",
      "Recall_Micro_Avg    = 0.93 \n",
      "Precision_Micro_Avg = 0.93\n",
      "Number of No-Yes SNP's :  3422\n",
      "Recall_Micro_Avg    = 0.97 \n",
      "Precision_Micro_Avg = 0.97\n",
      "Number of No-Yes SNP's :  3337\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4377\n",
      "Recall_Micro_Avg    = 0.79 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  3366\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.97\n",
      "Number of No-Yes SNP's :  4303\n",
      "Recall_Micro_Avg    = 0.86 \n",
      "Precision_Micro_Avg = 0.52\n",
      "Number of No-Yes SNP's :  4069\n",
      "Recall_Micro_Avg    = 0.87 \n",
      "Precision_Micro_Avg = 0.52\n",
      "Number of No-Yes SNP's :  2071\n",
      "Recall_Micro_Avg    = 0.92 \n",
      "Precision_Micro_Avg = 0.93\n",
      "Number of No-Yes SNP's :  2959\n",
      "Recall_Micro_Avg    = 0.92 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  1167\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1421\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1193\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  14032\n",
      "Recall_Micro_Avg    = 0.9 \n",
      "Precision_Micro_Avg = 0.91\n",
      "Number of No-Yes SNP's :  2180\n",
      "Recall_Micro_Avg    = 0.87 \n",
      "Precision_Micro_Avg = 0.52\n",
      "Number of No-Yes SNP's :  4086\n",
      "Recall_Micro_Avg    = 0.68 \n",
      "Precision_Micro_Avg = 0.68\n",
      "Number of No-Yes SNP's :  1073\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  10863\n",
      "Recall_Micro_Avg    = 0.91 \n",
      "Precision_Micro_Avg = 0.91\n",
      "Number of No-Yes SNP's :  1769\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3879\n",
      "Recall_Micro_Avg    = 0.84 \n",
      "Precision_Micro_Avg = 0.85\n",
      "Number of No-Yes SNP's :  1606\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6577\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1459\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  579\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  8174\n",
      "Recall_Micro_Avg    = 0.88 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  10184\n",
      "Recall_Micro_Avg    = 0.86 \n",
      "Precision_Micro_Avg = 0.86\n",
      "Number of No-Yes SNP's :  4341\n",
      "Recall_Micro_Avg    = 0.66 \n",
      "Precision_Micro_Avg = 0.69\n",
      "Number of No-Yes SNP's :  2662\n",
      "Recall_Micro_Avg    = 0.86 \n",
      "Precision_Micro_Avg = 0.52\n",
      "Number of No-Yes SNP's :  4207\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.54\n",
      "Number of No-Yes SNP's :  1842\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1637\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  5143\n",
      "Recall_Micro_Avg    = 0.68 \n",
      "Precision_Micro_Avg = 0.7\n",
      "Number of No-Yes SNP's :  4587\n",
      "Recall_Micro_Avg    = 0.84 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  1154\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1262\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  17848\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9243\n",
      "Recall_Micro_Avg    = 0.97 \n",
      "Precision_Micro_Avg = 0.97\n",
      "Number of No-Yes SNP's :  6344\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1033\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3762\n",
      "Recall_Micro_Avg    = 0.58 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  13440\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1894\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  5247\n",
      "Recall_Micro_Avg    = 0.65 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  8720\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9668\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  554\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1455\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  681\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4435\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1262\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1342\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1560\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1404\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  16714\n",
      "Recall_Micro_Avg    = 0.84 \n",
      "Precision_Micro_Avg = 0.86\n",
      "Number of No-Yes SNP's :  11915\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1344\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1719\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2388\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2147\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.94\n",
      "Number of No-Yes SNP's :  3281\n",
      "Recall_Micro_Avg    = 0.95 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  2964\n",
      "Recall_Micro_Avg    = 0.92 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  13462\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  12065\n",
      "Recall_Micro_Avg    = 0.9 \n",
      "Precision_Micro_Avg = 0.91\n",
      "Number of No-Yes SNP's :  1762\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  769\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9434\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2892\n",
      "Recall_Micro_Avg    = 0.92 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  666\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  785\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  17294\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1860\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2203\n",
      "Recall_Micro_Avg    = 0.91 \n",
      "Precision_Micro_Avg = 0.91\n",
      "Number of No-Yes SNP's :  1307\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6724\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3476\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.96\n",
      "Number of No-Yes SNP's :  2899\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9185\n",
      "Recall_Micro_Avg    = 0.72 \n",
      "Precision_Micro_Avg = 0.78\n",
      "Number of No-Yes SNP's :  1304\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  10656\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6756\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2538\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1595\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2214\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6440\n",
      "Recall_Micro_Avg    = 0.74 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  1134\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1350\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4375\n",
      "Recall_Micro_Avg    = 0.75 \n",
      "Precision_Micro_Avg = 0.75\n",
      "Number of No-Yes SNP's :  1460\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4199\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.54\n",
      "Number of No-Yes SNP's :  4312\n",
      "Recall_Micro_Avg    = 0.81 \n",
      "Precision_Micro_Avg = 0.54\n",
      "Number of No-Yes SNP's :  4087\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1900\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1331\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1301\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2549\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  781\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1387\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  5749\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.54\n",
      "Number of No-Yes SNP's :  581\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2350\n",
      "Recall_Micro_Avg    = 0.91 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  1389\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  10798\n",
      "Recall_Micro_Avg    = 0.97 \n",
      "Precision_Micro_Avg = 0.97\n",
      "Number of No-Yes SNP's :  8329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall_Micro_Avg    = 0.7 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  19995\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9027\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4002\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3000\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1490\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1654\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2109\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.94\n",
      "Number of No-Yes SNP's :  533\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  7766\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2189\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  12407\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  10937\n",
      "Recall_Micro_Avg    = 0.85 \n",
      "Precision_Micro_Avg = 0.85\n",
      "Number of No-Yes SNP's :  9122\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2025\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  674\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  987\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1448\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  5929\n",
      "Recall_Micro_Avg    = 0.88 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  648\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4188\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.54\n",
      "Number of No-Yes SNP's :  1556\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  5175\n",
      "Recall_Micro_Avg    = 0.79 \n",
      "Precision_Micro_Avg = 0.8\n",
      "Number of No-Yes SNP's :  1404\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  551\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2870\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1447\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  8883\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1319\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1147\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  634\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1468\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1801\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  8350\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  634\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  13482\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  990\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  22633\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2027\n",
      "Recall_Micro_Avg    = 0.92 \n",
      "Precision_Micro_Avg = 0.92\n",
      "Number of No-Yes SNP's :  1728\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1307\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1072\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1467\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1996\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3945\n",
      "Recall_Micro_Avg    = 0.73 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  12150\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  5315\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  13233\n",
      "Recall_Micro_Avg    = 0.88 \n",
      "Precision_Micro_Avg = 0.88\n",
      "Number of No-Yes SNP's :  2620\n",
      "Recall_Micro_Avg    = 0.6 \n",
      "Precision_Micro_Avg = 0.62\n",
      "Number of No-Yes SNP's :  5655\n",
      "Recall_Micro_Avg    = 0.98 \n",
      "Precision_Micro_Avg = 0.98\n",
      "Number of No-Yes SNP's :  2652\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  7997\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  20111\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3712\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9637\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1456\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1759\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  8797\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  773\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  12391\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  996\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  10095\n",
      "Recall_Micro_Avg    = 0.92 \n",
      "Precision_Micro_Avg = 0.93\n",
      "Number of No-Yes SNP's :  9017\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  713\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1839\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  14977\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4597\n",
      "Recall_Micro_Avg    = 0.95 \n",
      "Precision_Micro_Avg = 0.55\n",
      "Number of No-Yes SNP's :  10444\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  10551\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6460\n",
      "Recall_Micro_Avg    = 0.87 \n",
      "Precision_Micro_Avg = 0.55\n",
      "Number of No-Yes SNP's :  1467\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3682\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2591\n",
      "Recall_Micro_Avg    = 0.9 \n",
      "Precision_Micro_Avg = 0.52\n",
      "Number of No-Yes SNP's :  1551\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1457\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3068\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2729\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  1520\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1873\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  645\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  739\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  11062\n",
      "Recall_Micro_Avg    = 0.99 \n",
      "Precision_Micro_Avg = 0.99\n",
      "Number of No-Yes SNP's :  1570\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  5328\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  547\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4201\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  5654\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  747\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  831\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1625\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2640\n",
      "Recall_Micro_Avg    = 0.95 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  2121\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  707\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2049\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1083\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4111\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4644\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1686\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1973\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6549\n",
      "Recall_Micro_Avg    = 0.83 \n",
      "Precision_Micro_Avg = 0.84\n",
      "Number of No-Yes SNP's :  715\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3262\n",
      "Recall_Micro_Avg    = 0.8 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  1894\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  602\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1854\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  16993\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1408\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2655\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3056\n",
      "Recall_Micro_Avg    = 0.92 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  1582\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6782\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2433\n",
      "Recall_Micro_Avg    = 0.88 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  1842\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1280\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9039\n",
      "Recall_Micro_Avg    = 0.68 \n",
      "Precision_Micro_Avg = 0.72\n",
      "Number of No-Yes SNP's :  1672\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  637\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2179\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2206\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1119\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  8601\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  7553\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2463\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4756\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  800\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1204\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3408\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3208\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1575\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1067\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3231\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  19514\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3993\n",
      "Recall_Micro_Avg    = 0.59 \n",
      "Precision_Micro_Avg = 0.5\n",
      "Number of No-Yes SNP's :  1810\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1083\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  590\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1715\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1336\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1749\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3958\n",
      "Recall_Micro_Avg    = 0.97 \n",
      "Precision_Micro_Avg = 0.97\n",
      "Number of No-Yes SNP's :  2091\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.94\n",
      "Number of No-Yes SNP's :  1610\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1301\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4106\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1518\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2180\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.96\n",
      "Number of No-Yes SNP's :  1092\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3829\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  21430\n",
      "Recall_Micro_Avg    = 0.85 \n",
      "Precision_Micro_Avg = 0.51\n",
      "Number of No-Yes SNP's :  6491\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  9278\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2717\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1681\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2345\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.96\n",
      "Number of No-Yes SNP's :  1155\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2327\n",
      "Recall_Micro_Avg    = 0.78 \n",
      "Precision_Micro_Avg = 0.78\n",
      "Number of No-Yes SNP's :  3713\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  10841\n",
      "Recall_Micro_Avg    = 0.98 \n",
      "Precision_Micro_Avg = 0.98\n",
      "Number of No-Yes SNP's :  1568\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  8209\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  18354\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1655\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2669\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1762\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1500\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  491\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1255\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1770\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1396\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2256\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  695\n",
      "Passed !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of No-Yes SNP's :  1655\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6905\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3412\n",
      "Recall_Micro_Avg    = 0.97 \n",
      "Precision_Micro_Avg = 0.97\n",
      "Number of No-Yes SNP's :  593\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  987\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  594\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  10396\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  3143\n",
      "Recall_Micro_Avg    = 0.96 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  4174\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  5673\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  725\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  978\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2680\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  543\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2533\n",
      "Recall_Micro_Avg    = 0.94 \n",
      "Precision_Micro_Avg = 0.53\n",
      "Number of No-Yes SNP's :  8061\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4575\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  791\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  1554\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2082\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  4869\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  2519\n",
      "Recall_Micro_Avg    = 0.97 \n",
      "Precision_Micro_Avg = 0.97\n",
      "Number of No-Yes SNP's :  1729\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  6575\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  8975\n",
      "Passed !\n",
      "Number of No-Yes SNP's :  8388\n",
      "Recall_Micro_Avg    = 0.98 \n",
      "Precision_Micro_Avg = 0.98\n"
     ]
    }
   ],
   "source": [
    "experiments = [(1,50,1,1),(2,50,0.85,0.7),(3,50,0.8,0.45)]\n",
    "experiments = [(2,50,0.85,0.7)]\n",
    "\n",
    "for e in experiments:\n",
    "    print(\"\\n\\nSTARTED\")\n",
    "    victim_set = {}\n",
    "    used_victims = []\n",
    "    gc.collect()\n",
    "    add_count = e[0]\n",
    "    cluster_count = e[0]\n",
    "    beacon_size = e[1]\n",
    "    target = e[2]\n",
    "    for i in range(40):\n",
    "        precision, recall = 0, 0\n",
    "        while precision + recall < target * 2:\n",
    "            _, no_yes_ind, added_people     = getNoYes(add_count, beacon_size)\n",
    "            if (set(added_people) & set(used_victims)) or len(no_yes_ind) > 25000 or len(no_yes_ind) < 2000:\n",
    "                print(\"Passed !\")\n",
    "                continue\n",
    "            correlations                              = builtSNPNetwork(no_yes_ind, pheno5People, reference)\n",
    "            reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "            (precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)\n",
    "        for a in added_people:\n",
    "            used_victims.append(a)\n",
    "        victim_set[i] = [added_people, no_yes_ind, matches, reconstructed_spectral]\n",
    "    with open(join(testSets, \"mi2_victims_50_\"+str(e[0])+\".pkl\"), 'wb') as f:\n",
    "        pickle.dump(victim_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in experiments:\n",
    "    with open(join(testSets, \"mi2_victims_50_\"+str(e[0])+\".pkl\"), 'rb') as f:\n",
    "        victim_set = pickle.load(f)\n",
    "    \n",
    "    case_genomes = []\n",
    "    control_genomes = []\n",
    "\n",
    "    # Case\n",
    "    case_people = []\n",
    "    for i in range(20):\n",
    "        victim = victim_set[i]\n",
    "        case_people.append(victim[0][0])\n",
    "        if i < e[3]*20:\n",
    "            case_genomes.append(victim[3][victim[2][0]]) # match\n",
    "        else:\n",
    "            case_genomes.append(victim[3][victim[2][1]]) # mismatch\n",
    "    case_people = np.array(case_people)\n",
    "    case_genomes = pd.DataFrame(data=np.array(case_genomes).T, columns=case_people, index=beacon.index)\n",
    "\n",
    "    # Control\n",
    "    control_people = []\n",
    "    for i in range(20,40):\n",
    "        victim = victim_set[i]\n",
    "        control_people.append(victim[0][0])\n",
    "        if i < e[3]*20+20:\n",
    "            control_genomes.append(victim[3][victim[2][0]])\n",
    "        else:\n",
    "            control_genomes.append(victim[3][victim[2][1]])\n",
    "    control_people = np.array(control_people)\n",
    "    control_genomes = pd.DataFrame(data=np.array(control_genomes).T, columns=control_people, index=beacon.index)\n",
    "    \n",
    "    # Beacon\n",
    "    victims = np.concatenate([case_people, control_people])\n",
    "    beacon_candidates = np.setdiff1d(phenoAllPeople, victims)\n",
    "\n",
    "    # CASE\n",
    "    random.shuffle(beacon_candidates)\n",
    "    cb_ind = beacon_candidates[:40]\n",
    "    cb_ind = np.concatenate([case_people, cb_ind])\n",
    "    case_beacon = beacon.iloc[:, cb_ind]\n",
    "\n",
    "    # CONTROL\n",
    "    random.shuffle(beacon_candidates)\n",
    "    cb_ind = beacon_candidates[:60]\n",
    "    control_beacon = beacon.iloc[:, cb_ind]\n",
    "    \n",
    "    # Save\n",
    "    case_genomes.to_csv(join(inferencePath, \"CaseGenomes_2\"+str(e[0])+\".txt\"), sep=\"\\t\")\n",
    "    control_genomes.to_csv(join(inferencePath, \"ControlGenomes_2\"+str(e[0])+\".txt\"), sep=\"\\t\")\n",
    "    case_beacon.to_csv(join(inferencePath, \"Beacon_2\"+str(e[0])+\".txt\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAF Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(opensnpPath, \"MAF.pickle\"), 'rb') as handle:\n",
    "    maf = pickle.load(handle)\n",
    "maf['major'],maf['minor']=np.where(maf['maf']==maf['major_freq'],(maf['minor'],maf['major']),(maf['major'],maf['minor']))\n",
    "maf['major_freq'],maf['minor_freq']=np.where(maf['maf']==maf['major_freq'],(maf['minor_freq'],maf['major_freq']),(maf['major_freq'],maf['minor_freq']))\n",
    "\n",
    "maf = maf.rename(columns={'chr': 'chromosome', 'major': 'referenceAllele', 'major_freq': 'referenceAlleleFrequency', \n",
    "                          'minor': 'otherAllele', 'minor_freq': 'otherAlleleFrequency', 'count': 'position'})\n",
    "maf[\"markerId\"] = maf.index\n",
    "del maf[\"maf\"]\n",
    "cols = np.concatenate([maf.columns[:2].values, maf.columns[-1:], maf.columns[2:-1].values])\n",
    "maf = maf[cols]\n",
    "maf.index = np.arange(len(maf))\n",
    "maf.to_csv(join(inferencePath, \"MAF.txt\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "i = 0\n",
    "ll = victim_set.keys()\n",
    "for k in ll:\n",
    "    victim_set[i] = victim_set.pop(k)\n",
    "    i+=1\n",
    "for k, v in victim_set.items():\n",
    "    print(k, \": \", v[0])\n",
    "len(np.unique(case_victims)) == len(case_victims)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def train_models(train_snps, test_people):\n",
    "    models = []\n",
    "    count = 1\n",
    "    for feature in features:\n",
    "        X, Y = getTrainingData(phenotype=feature, pos=train_snps, test_people=test_people)\n",
    "        print(\"\\n\",count, \".\", feature, \"\\tlabels=\", np.unique(Y), \"\\t\",end=\"\", flush=True)\n",
    "\n",
    "        X, Y = SMOTE().fit_sample(X, Y)\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=4,criterion='entropy',class_weight='balanced_subsample',max_features=X.shape[1]//4,\n",
    "                                       min_samples_leaf=4,bootstrap=True,verbose=0,n_jobs=-1,oob_score=True)\n",
    "    \n",
    "        #model = GridSearchCV(cv=10, estimator=rf, scoring='f1_macro', param_grid=parameters,verbose=2,n_jobs=-1)\n",
    "        result = model.fit(X, Y)\n",
    "\n",
    "        #print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "        #best_estimator = result.best_estimator_\n",
    "        #best_estimator.fit(X, Y)\n",
    "        print(model.oob_score_)\n",
    "        if model.oob_score_ > 1.2 / len(np.unique(Y)):\n",
    "            count += 1\n",
    "            print(\"\\nTrain:\", round(model.score(X, Y), 2), \" | Out-of-Bag:\", round(model.oob_score_,2))\n",
    "            models.append((feature, model, model.oob_score_))\n",
    "    return models\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def train_models(train_snps, test_people):\n",
    "    models = []\n",
    "    count = 1\n",
    "    for feature in features:\n",
    "        X, Y = getTrainingData(phenotype=feature, pos=train_snps, test_people=test_people)\n",
    "        print(\"\\n\",count, \".\", feature, \"\\tlabels=\", np.unique(Y), \"\\t\",end=\"\", flush=True)\n",
    "\n",
    "        X, Y = SMOTE().fit_sample(X, Y)\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=16,criterion='entropy',class_weight='balanced_subsample',max_features=X.shape[1]//2,\n",
    "                                       min_samples_leaf=2,bootstrap=True,verbose=0,n_jobs=-1,oob_score=True)\n",
    "    \n",
    "        '''model = BalancedRandomForestClassifier(n_estimators=100, max_depth=16, min_samples_split=2, min_samples_leaf=2, min_weight_fraction_leaf=0,\n",
    "                                            max_features=None, max_leaf_nodes=None, bootstrap=True, oob_score=True, replacement=False,\n",
    "                                            n_jobs=-1, warm_start=False, criterion='entropy', class_weight=\"balanced_subsample\")'''\n",
    "        \n",
    "        model.fit(X, Y)\n",
    "        if model.oob_score_ > 1.2 / len(np.unique(Y)):\n",
    "            count += 1\n",
    "            print(\"\\nTrain:\", round(model.score(X, Y), 2), \" | Out-of-Bag:\", round(model.oob_score_,2))\n",
    "            models.append((feature, model, model.oob_score_))\n",
    "    return models\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def train_models(train_snps, test_people):\n",
    "    models = []\n",
    "    count = 1\n",
    "    for feature in features:\n",
    "        X, Y = getTrainingData(phenotype=feature, pos=train_snps, test_people=test_people)\n",
    "\n",
    "        print(\"\\n\",count, \".\", feature, \"\\tlabels=\", np.unique(Y), \"\\t\",end=\"\", flush=True)\n",
    "        \n",
    "        for epoch in range(25):\n",
    "            # Train/Val Split\n",
    "            x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, shuffle=True, stratify=Y)\n",
    "            \n",
    "            # Upsampling\n",
    "            x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "            \n",
    "            # Train the model\n",
    "            model = RandomForestClassifier(n_estimators=100,max_depth=4,min_samples_leaf=8,criterion=\"entropy\",class_weight='balanced_subsample',bootstrap=True,verbose=0,n_jobs=-1)\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred = model.predict(x_val)\n",
    "\n",
    "            # Performance\n",
    "            result = classification_report(y_val, y_pred, output_dict=True)\n",
    "            isBetter = result[\"macro avg\"][\"f1-score\"] > 1.2 / len(np.unique(y_train))\n",
    "            if isBetter:\n",
    "                count += 1\n",
    "                print(\"\\nTrain:\", round(model.score(x_train, y_train), 2), \" | Test:\", round(model.score(x_val, y_val),2))\n",
    "                print(classification_report(y_val, y_pred, output_dict=False))\n",
    "                #model.fit(np.concatenate([x_train, x_val], axis=0), np.concatenate([y_train, y_val], axis=0))\n",
    "                models.append((feature, model, result[\"macro avg\"][\"f1-score\"]))\n",
    "                break\n",
    "            print(\"|\", end=\"\", flush=True)\n",
    "    return models\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Reconstruction average performans iyi değil\n",
    "2. Bilmemiz gereken fenotip sayısı > 10-15\n",
    "3. Performans nasıl report edeceğiz ?\n",
    "4. Top-1 olmazsa nasıl Membership kısmına bağlayacağız\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Stash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# SINGLE MODELS\n",
    "model = XGBClassifier(objective=\"multi:softprob\",eval_metric=\"auc\",num_class=len(np.unique(y_train)),n_jobs=-1,learning_rate=0.001,tree_method=\"hist\",\n",
    "                          gamma=3,reg_lambda=10,max_depth=10,max_delta_step=1,colsample_bytree=0.95,scale_pos_weight=10000,num_parallel_tree=8,booster=\"dart\")\n",
    "\n",
    "model = BalancedRandomForestClassifier(n_estimators=150, max_depth=None, min_samples_split=5, min_samples_leaf=2, min_weight_fraction_leaf=0,\n",
    "                                            max_features='auto', max_leaf_nodes=None, bootstrap=True, oob_score=False, replacement=False,\n",
    "                                            n_jobs=-1, warm_start=True, class_weight=\"balanced\")\n",
    "        \n",
    "model = LogisticRegression(penalty='l1',random_state=0,solver='saga',multi_class='multinomial',n_jobs=-1,C=10,max_iter=100)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=128, max_depth=8,class_weight='balanced_subsample',verbose=0,n_jobs=-1)\n",
    "\n",
    "model = BalancedBaggingClassifier()\n",
    "\n",
    "# PIPELINE\n",
    "selecter = SelectKBest(chi2, k=20000)\n",
    "xgb = XGBClassifier(objective=\"multi:softprob\",eval_metric=\"error\",num_class=len(np.unique(y_train)),n_jobs=-1,\n",
    "                      learning_rate=0.05, gamma=1, max_depth=20,subsample=1, colsample_bytree=1, scale_pos_weight=10000, num_parallel_tree=32)\n",
    "estimators = [('selection', selecter), ('brc', xgb)]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# SAMPLING METHODS\n",
    "smote = SMOTE()\n",
    "x_train, y_train = smote.fit_sample(x_train, y_train)\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "x_train, y_train = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "tom = TomekLinks(ratio=\"majority\")\n",
    "x_train, y_train = tom.fit_sample(x_train, y_train)\n",
    "\n",
    "cc = ClusterCentroids()\n",
    "x_train, y_train = cc.fit_sample(x_train, y_train)\n",
    "\n",
    "\n",
    "# GRID SEARCH                                            \n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1)\n",
    "model = RandomizedSearchCV(cv=5, estimator=xgboost, param_distributions=parameters,n_iter=100,verbose=10,n_jobs=-1)\n",
    "\n",
    "\n",
    "# SAVE MODELS\n",
    "with open(join(models, 'Model_' + feature + '.pkl'), 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "\n",
    "Gammas    = np.linspace(0, 10, num=11)           # gamma\n",
    "Depths    = np.linspace(4, 10, num=7, dtype=int) # max_depth\n",
    "Deltas    = np.logspace(0, 4, num=5, base=2)     # max_delta_step\n",
    "Learning  = np.logspace(-3, 0, num=4)            # learning_rate\n",
    "Lambdas   = np.logspace(-3, 0, num=4)            # lambda\n",
    "MinChild  = np.logspace(0, 5, num=6, base=2)     # min_child_weight\n",
    "Scale     = np.logspace(0, 6, num=7)             # scale_pos_weight\n",
    "Subsample = [1, 0.75]                            # subsample\n",
    "ColSample = [1, 0.75]                            # colsample_bytree\n",
    "Forest    = [100]                                # num_parallel_tree\n",
    "\n",
    "parameters = {\"learning_rate\":Learning, \"gamma\":Gammas, \"max_depth\":Depths, \n",
    "              \"max_delta_step\":Deltas, \"lambda\":Lambdas, \"min_child_weight\":MinChild, \n",
    "              \"subsample\":Subsample, \"colsample_bytree\":ColSample, \"scale_pos_weight\":Scale,\n",
    "              \"num_parallel_tree\":Forest}\n",
    "              \n",
    "Estimators= np.logspace(2, 4, num=3, dtype=int)  # n_estimators\n",
    "Depths    = np.linspace(4, 10, num=7, dtype=int) # max_depth (None olabilir)\n",
    "MinSplit  = np.linspace(2, 8, num=7, dtype=int)  # min_samples_split\n",
    "MinSample = np.linspace(1, 5, num=6, dtype=int)  # min_samples_leaf\n",
    "Impurity  = np.logspace(0, 6, num=7, dtype=int)  # min_impurity_decrease\n",
    "Criterion = [\"gini\", \"entropy\"]                  # criterion\n",
    "\n",
    "parameters = {\"max_depth\":Depths, \"min_samples_split\":MinSplit, \"min_samples_leaf\":MinSample, \n",
    "              \"min_impurity_decrease\":Impurity, \"criterion\":Criterion, \"n_estimators\":Estimators}\n",
    "              \n",
    "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "              \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(join(opensnpPath, \"QuadBeacon.pickle\"), 'rb') as handle:\n",
    "    quad = pickle.load(handle)\n",
    "# 0: one minor 1: double minor 2: double major 3: NN\n",
    "\n",
    "gs = [yes_yes_ind, no_yes_ind, added_people]\n",
    "with open(join(beacons, \"goodsetup.pkl\"), 'wb') as f:\n",
    "    pickle.dump(gs, f)\n",
    "\n",
    "with open(join(beacons, \"goodsetup.pkl\"), 'rb') as f:\n",
    "    yes_yes_ind, no_yes_ind, added_people = pickle.load(f)\n",
    "yes_yes_ind, no_yes_ind, added_people\n",
    "\n",
    "#original_x = binary[no_yes_ind][:,added_people]\n",
    "#test_x = original_x.T\n",
    "#matches2 = np.arange(10)\n",
    "\n",
    "#from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, TomekLinks\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, LeakyReLU, Dropout\n",
    "#from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "def train_models2(train_snps, test_people, parameters):\n",
    "    models = []\n",
    "    for feature in features:\n",
    "        X, Y = getTrainingData(phenotype=feature, pos=train_snps, test_people=test_people)\n",
    "\n",
    "        print(\"\\n\", feature, \"\\tlabels=\", np.unique(Y), \"\\t\",end=\"\", flush=True)\n",
    "\n",
    "        for epoch in range(1):\n",
    "            # Upsampling\n",
    "            smote = SMOTE()\n",
    "            X, Y = smote.fit_sample(X, Y)\n",
    "            \n",
    "            # Train the model\n",
    "            rf = RandomForestClassifier(n_estimators=100, max_depth=16,class_weight='balanced_subsample',verbose=0,n_jobs=-1)\n",
    "            model = RandomizedSearchCV(cv=10, estimator=rf, param_distributions=parameters,n_iter=100,verbose=10,n_jobs=-1)\n",
    "            result = model.fit(X, Y)\n",
    "            \n",
    "            print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "\n",
    "            best_model = model.best_estimator_\n",
    "            models.append((feature, best_model, np.mean(result.cv_results_['mean_test_score']), result.cv_results_))\n",
    "        break\n",
    "    return models\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot_confusion_matrix(cm=np.array([[tp_total,fp_total],\n",
    "                                   [fn_total,tn_total]]),\n",
    "                  target_names=['mutation', 'normal'],\n",
    "                  title=\"Confusion Matrix\")\n",
    "   \n",
    "# Use rare indices or not ? \n",
    "threshold = 0.01\n",
    "condition = np.logical_and(maf['maf'] < threshold, maf['maf'] > 0)\n",
    "rare_percent = maf[condition].shape[0] / len(giant) * 100\n",
    "rare_indices = np.where(condition==True)[0]\n",
    "rare_names = maf[condition].index.values\n",
    "print(len(rare_indices))\n",
    "\n",
    "\n",
    "r = small.columns[np.random.choice(len(small.columns), size=45, replace=False)]\n",
    "    \n",
    "%%time\n",
    "# Set NN to MAF values\n",
    "for i in range(mutation_beacon.shape[0]):\n",
    "    mutation_beacon[i][ny_beacon[ind].values[i] == \"NN\"] = maf.iloc[no_yes_ind][\"maf\"][i]\n",
    "mutation_beacon\n",
    "\n",
    "beacon = pd.read_csv(join(opensnpPath, \"Beacon.csv\"),sep=',',dtype=\"category\",header=None)\n",
    "\n",
    "le = LabelEncoder()\n",
    "beacon.apply(le.fit_transform)\n",
    "\n",
    "# Confusion matrix plotter method\n",
    "def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def onehot_initialization_v3(a):\n",
    "    ncols = a.max() + 1\n",
    "    labels_one_hot = (a.ravel()[np.newaxis] == np.arange(ncols)[:, np.newaxis]).T\n",
    "    labels_one_hot.shape = a.shape + (ncols,)\n",
    "    return labels_one_hot\n",
    "\n",
    "x = onehot_initialization_v3(quad.values.T)\n",
    "x = x.astype(np.int8)\n",
    "# Smoothen\n",
    "    for t in range(len(results)):\n",
    "        idx = np.argmax(results[t], axis=-1)\n",
    "        results[t] = np.zeros(results[t].shape )\n",
    "        results[t][np.arange(results[t].shape[0]), idx] = 1\n",
    "        \n",
    "ny_snps = binary[no_yes_ind][:,added_people].T\n",
    "matches = np.arange(add_count)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## 3. Mutual Information\n",
    "def get_pvalues(X, tempy):\n",
    "    t11 = np.sum(X.T * tempy, axis=1)\n",
    "    t10 = np.sum(X.T * (1-tempy), axis=1)\n",
    "    t01 = np.sum((1-X.T) * tempy, axis=1)\n",
    "    t00 = np.sum((1-X.T) * (1-tempy), axis=1)\n",
    "    t = np.array([np.array([t00[i], t01[i], t10[i], t11[i]]).reshape(2,2) for i in range(X.shape[1])])\n",
    "    values = np.array([stats.fisher_exact(i)[0] for i in t])\n",
    "    probs = np.nan_to_num(values / (1+values), nan=1)\n",
    "    probs[probs == 0] = 1e-8\n",
    "    return probs\n",
    "\n",
    "def train_mi(train_snps, test_people):\n",
    "    m_infos = []\n",
    "    for feature in features:\n",
    "        X, Y = getTrainingData(phenotype=feature, pos=train_snps, test_people=test_people)\n",
    "\n",
    "        print(feature, end=\"\", flush=True)\n",
    "        labels = np.unique(Y)\n",
    "        mis = np.zeros((len(labels), X.shape[1]))\n",
    "        for i in range(len(labels)):\n",
    "            tempy = Y.copy()\n",
    "            tempy[Y != labels[i]] = 0\n",
    "            tempy[Y == labels[i]] = 1\n",
    "            mis[i] = get_pvalues(X, tempy)\n",
    "            #tempy[tempy != labels[i]] = \"Other\"\n",
    "            #mis[i] = mutual_info_classif(X, tempy, discrete_features='auto', n_neighbors=3, copy=True)\n",
    "        m_infos.append((feature,mis))\n",
    "    return m_infos\n",
    "\n",
    "def test_mi(mis, x_test, y_test):\n",
    "    correct = 0\n",
    "    # For each person\n",
    "    for i in range(len(y_test)):\n",
    "        test_person = y_test.iloc[i]\n",
    "        scores = np.ones((len(y_test)), dtype=float)\n",
    "        # For each reconstructed genome\n",
    "        for j in range(len(y_test)):\n",
    "            available_phenotypes = np.where(test_person != \"-\")[0]\n",
    "            for k in available_phenotypes:\n",
    "                label = test_person[k]\n",
    "                available_labels = np.setdiff1d(pheno.iloc[:, k], \"-\")\n",
    "                pos = np.where(available_labels == label)[0]\n",
    "                scores[j] += np.mean(mis[k][1][:, x_test[j]][pos])\n",
    "                #scores[j] += np.log(np.mean(1+1e-8-mis[k][1][:, 1-x_test[j]][pos]))\n",
    "\n",
    "        print(scores)\n",
    "        matched_ind = np.argsort(scores)[-3:]\n",
    "        print(matched_ind, \"--\", matches[i])\n",
    "        print()\n",
    "        if matches[i] in matched_ind:\n",
    "            correct += 1\n",
    "    return correct / len(y_test)\n",
    " \n",
    "# Phenotype Prediction\n",
    "x_test = (reconstructed_spectral != reference.T[0])[:, no_yes_ind]\n",
    "y_test = pheno.loc[beacon.columns[added_people]]\n",
    "print(\"Set: \", i+1)\n",
    "mis = train_mi(train_snps=no_yes_ind, test_people=added_people)\n",
    "accuracy = test_mi(mis, x_test, y_test)\n",
    "print(\"Accuracy = \", accuracy)\n",
    "overall_accuracy.append(accuracy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Deep Learning\n",
    "models = []\n",
    "count = 0\n",
    "random.shuffle(features)\n",
    "for feature in features:\n",
    "    if feature == \"Sex\":\n",
    "        continue\n",
    "    # Find indices of people who has the specified feature\n",
    "    feature_label = pheno[pheno[feature] != \"-\"][feature]\n",
    "    existing = beacon.columns.isin(feature_label.index.values)\n",
    "    existing[added_people] = False \n",
    "    \n",
    "    X = binary[no_yes_ind][:, existing].T\n",
    "    Y = feature_label[beacon.columns[existing]].values\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    y = enc.fit_transform(Y.reshape(-1, 1)).toarray()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)\n",
    "    \n",
    "    # Train / Test\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=X.shape[1], activation=LeakyReLU(alpha=0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(np.unique(Y)), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'AUC'])\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=len(x_train))\n",
    "    y_pred = model.predict_classes(x_test, verbose=0)\n",
    "\n",
    "    # Performance\n",
    "    result = classification_report(np.where(y_test)[1], y_pred, output_dict=True)\n",
    "    isBetter = result[\"macro avg\"][\"f1-score\"] > 1.0 / len(np.unique(y_train))\n",
    "    if isBetter:\n",
    "        count += 1\n",
    "        print(count, \".\", feature, \" --> \", np.unique(Y))\n",
    "        #print(\"Train:\", round(model.score(x_train, y_train), 2), \" | Test:\", round(model.score(x_test, y_test),2))\n",
    "        print(round(result[\"macro avg\"][\"f1-score\"], 2), \">\" , 1.0 / len(np.unique(y_train)), \"\\n\")\n",
    "        models.append((feature, model, result[\"macro avg\"][\"f1-score\"]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 2. Including Yes-Yes SNP's\n",
    "isRandom = False\n",
    "percentage = 2\n",
    "if isRandom:\n",
    "    yy_count = len(yes_yes_ind) * percentage // 100\n",
    "    yy_pos = np.random.choice(yes_yes_ind, yy_count, replace=False)  \n",
    "    train_ind = np.sort(np.concatenate([yy_pos, no_yes_ind]))\n",
    "else:\n",
    "    yy_count = len(yes_yes_ind) * percentage // 100\n",
    "    yy_pos = yes_yes_ind[np.argsort(np.var(ternary[yes_yes_ind], axis=1))[::-1]][:yy_count]\n",
    "    train_ind = np.sort(np.concatenate([yy_pos, no_yes_ind]))\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "# Get no-yes reconstructed snps\n",
    "ny_pos  = np.where(np.in1d(train_ind, no_yes_ind))[0]\n",
    "ny_snps = reconstructed_spectral[:, no_yes_ind]\n",
    "ny_snps = np.logical_and(ny_snps == reference[no_yes_ind].T, ny_snps != \"NN\")\n",
    "ny_snps = ny_snps.astype(np.int8)\n",
    "\n",
    "correct = 0\n",
    "labels = [i[0] for i in models]\n",
    "test_y = pheno.loc[beacon.columns[added_people]]\n",
    "\n",
    "# For each person\n",
    "for i in range(len(test_y)):\n",
    "    test_person = test_y[labels].iloc[i]\n",
    "    # Predict each cluster\n",
    "    results = []\n",
    "    test_x = binary[train_ind][:, added_people[i]]\n",
    "    test_x = np.expand_dims(test_x, axis=0)\n",
    "    test_x = np.repeat(test_x,add_count,axis=0)\n",
    "    test_x[:, ny_pos] = ny_snps\n",
    "    # For each model\n",
    "    for m in models:\n",
    "        results.append(m[1].predict_proba(test_x))\n",
    "    \n",
    "    # For each reconstructed genome\n",
    "    probs = np.zeros((len(test_y)))\n",
    "    for j in range(len(test_y)):\n",
    "        available_phenotypes = np.where(test_person != \"-\")[0]\n",
    "        # For each available phenotype\n",
    "        for k in available_phenotypes:\n",
    "            target_label_ind = np.where(models[k][1].classes_ == test_person[k])[0]\n",
    "            probs[j] += results[k][j][target_label_ind]\n",
    "    \n",
    "    print(probs)\n",
    "    # Top k\n",
    "    matched_ind = np.argsort(probs)[-3:]\n",
    "    print(matched_ind, \"--\", matches[i])\n",
    "    print()\n",
    "    if matches[i] in matched_ind:\n",
    "        correct += 1\n",
    "    \n",
    "acc = correct / len(test_y)\n",
    "acc\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add_count = 2\n",
    "cluster_count = 2\n",
    "for i in range(40):\n",
    "    no_yes_ind = victim_set[i][1]\n",
    "    added_people = victim_set[i][0]\n",
    "    model_ind = np.setdiff1d(pheno1People, added_people)\n",
    "    correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "    reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "    (precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)\n",
    "\n",
    "    victim_set[i][2] = matches\n",
    "    victim_set[i][3] = reconstructed_spectral\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
