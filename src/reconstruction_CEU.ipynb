{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import timeit\n",
    "import itertools\n",
    "import warnings\n",
    "import pickle\n",
    "import feather\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join, isfile\n",
    "from collections import Counter\n",
    "from fcmeans import FCM\n",
    "import scipy.stats as stats\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error, classification_report, mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainPath = \"../../data\"\n",
    "beacons = join(mainPath, \"beacon\")\n",
    "testSets = join(beacons, \"testsets\")\n",
    "models = join(mainPath, \"models\")\n",
    "ceuPath = join(beacons, \"CEU\")\n",
    "opensnpPath = join(beacons, \"OpenSNP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Load Beacon, MAF, Reference and other cached variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEU\n",
    "beacon = pd.read_csv(join(ceuPath, \"Beacon_164.txt\"), index_col=0, delim_whitespace=True)\n",
    "maf = pd.read_csv(join(ceuPath, \"MAF.txt\"), index_col=0, delim_whitespace=True)\n",
    "reference = pickle.load(open(join(ceuPath, \"reference.pickle\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = np.logical_and(beacon.values != reference, beacon.values != \"NN\").astype(int)\n",
    "ternary = binary.copy()\n",
    "ternary[beacon.values==\"NN\"] = -1\n",
    "\n",
    "maf.rename(columns = {'referenceAllele':'major', 'referenceAlleleFrequency':'major_freq', 'otherAllele':'minor', 'otherAlleleFrequency':'minor_freq'}, inplace = True)\n",
    "\n",
    "beaconPeople = np.arange(65)\n",
    "otherPeople = np.arange(99)+65\n",
    "allPeople = np.arange(164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary = np.zeros(beacon.shape, dtype=int)\n",
    "ternary[beacon != reference] = 1\n",
    "ternary[beacon == \"NN\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "tr1 = pca.fit_transform(ternary.T)\n",
    "plt.scatter(tr1[:, 0], tr1[:, 1], alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 81, 1: 75, 2: 8})\n",
      "[[-976.14 11.53 -0.02 -0.02]\n",
      " [938.20 -171.92 0.20 0.06]\n",
      " [1087.75 1495.00 -1.68 -0.29]]\n",
      "(81,) (75,) (8,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmeans = KMeans(3)\n",
    "y_kmeans = kmeans.fit_predict(tr1)\n",
    "plt.scatter(tr1[:, 0], tr1[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "print(Counter(y_kmeans))\n",
    "print(centers)\n",
    "\n",
    "# Get indices of homogeneous groups\n",
    "g1 = allPeople[np.where(y_kmeans == 0)[0]]\n",
    "g2 = allPeople[np.where(y_kmeans == 1)[0]]\n",
    "g3 = allPeople[np.where(y_kmeans == 2)[0]]\n",
    "print(g1.shape, g2.shape, g3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chromosome Seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_index = 1\n",
    "ind = (maf[\"chromosome\"] == \"chr\"+str(chromosome_index)).values\n",
    "print(np.sum(ind),\" SNP's exist in chromosome \", chromosome_index)\n",
    "\n",
    "reference = reference[ind]\n",
    "beacon = beacon.loc[ind]\n",
    "extra = extra.loc[ind]\n",
    "giant = giant.loc[ind]\n",
    "maf = maf.loc[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.2: Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beacon operations\n",
    "def queryBeacon(beacon_people):\n",
    "    return binary[:, beacon_people].any(axis=1)\n",
    "\n",
    "def getMutationAt(index):\n",
    "    temp = maf.iloc[index]\n",
    "    if temp[\"minor_freq\"] == temp[\"maf\"]:\n",
    "        return temp[\"minor\"] + temp[\"minor\"] \n",
    "    else:\n",
    "        return temp[\"major\"] + temp[\"major\"] \n",
    "\n",
    "def div(n, d):\n",
    "    return n / d if d else 0\n",
    "\n",
    "def rpaCalculate(tp,fp,tn,fn):\n",
    "    recall = div(tp,(tp+fn)) \n",
    "    precision = div(tp,(tp+fp))\n",
    "    accuracy = div((tp+tn),(tp+fp+tn+fn))\n",
    "    return recall, precision, accuracy\n",
    "\n",
    "# Performance method\n",
    "def performance(person, reconstruction, eval_pos, reference):\n",
    "    ind = np.logical_and(person[eval_pos] != np.squeeze(reference)[eval_pos], person[eval_pos] != \"NN\")\n",
    "    tp = np.sum(reconstruction[eval_pos][ind] != np.squeeze(reference)[eval_pos][ind])\n",
    "    fn = np.sum(ind) - tp\n",
    "    fp = np.sum(reconstruction[eval_pos][~ind] != np.squeeze(reference)[eval_pos][~ind])\n",
    "    tn = np.sum(~ind) - fp\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def performance_f(test_people, reconstructed, add_count, cluster_count, eval_pos):\n",
    "    total_values = np.zeros((4))\n",
    "    best_matches = []\n",
    "    # For all people in victim set\n",
    "    for i in range(add_count):\n",
    "        all_combinations = np.zeros((4, cluster_count))\n",
    "        rpa = np.zeros((3, cluster_count))\n",
    "        # For each cluster obtained\n",
    "        for j in range(cluster_count):\n",
    "            all_combinations[:, j] = performance(test_people[i], reconstructed[j], eval_pos, reference)\n",
    "            rpa[:, j] = rpaCalculate(*all_combinations[:, j])\n",
    "        ind = np.argmax(rpa[0,:]*rpa[1,:])       #Best-match index\n",
    "        best_matches.append(ind)\n",
    "        total_values += all_combinations[:, ind] #Add total tp-fp-tn-fn\n",
    "    recall, precision, accuracy = rpaCalculate(*total_values)\n",
    "    print(\"Recall_Micro_Avg    =\", round(recall, 2),\"\\nPrecision_Micro_Avg =\", round(precision, 2))\n",
    "    return (precision,recall,accuracy), total_values, best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Choose random people and send query to Beacon to obtain No-Yes answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoYes(add_count, beacon_size):\n",
    "\n",
    "    # Take people for added group\n",
    "    added_people = otherPeople.copy()\n",
    "    random.shuffle(added_people)\n",
    "    added_people = added_people[:add_count]\n",
    "    \n",
    "    # Take people for beacon\n",
    "    beacon_people = np.setdiff1d(allPeople, added_people)\n",
    "    random.shuffle(beacon_people)\n",
    "    beacon_people = beacon_people[:beacon_size]\n",
    "\n",
    "    # Query Beacon initially\n",
    "    before = queryBeacon(beacon_people)\n",
    "    # Add people\n",
    "    updated_beacon = np.concatenate([added_people,beacon_people])\n",
    "    # Query Beacon again\n",
    "    after = queryBeacon(updated_beacon)\n",
    "    # Find No-Yes SNPs' indices\n",
    "    no_yes_indices = np.where(np.logical_and(before==False, after==True))[0]\n",
    "    yes_yes_indices = np.where(np.logical_and(before==True, after==True))[0]\n",
    "    print(\"Number of No-Yes SNP's : \", len(no_yes_indices))\n",
    "    \n",
    "    return yes_yes_indices, no_yes_indices, added_people, beacon_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoYes2(add_count, beacon_size):\n",
    "\n",
    "    # Take people for added group\n",
    "    random.shuffle(g1)\n",
    "    random.shuffle(g2)\n",
    "    if add_count == 5:\n",
    "        added_people = np.concatenate([g1[:3], g2[:2]])\n",
    "    else:\n",
    "        added_people = np.concatenate([g1[:(add_count // 2)], g2[:(add_count // 2)]])\n",
    "\n",
    "    # Take people for beacon\n",
    "    g1_ = np.setdiff1d(g1, added_people)\n",
    "    random.shuffle(g1_)\n",
    "    g2_ = np.setdiff1d(g2, added_people)\n",
    "    random.shuffle(g2_)\n",
    "    \n",
    "    \n",
    "    curBeacon = np.concatenate([g1_[:(beacon_size // 2)], g2_[:(beacon_size // 2)]])\n",
    "\n",
    "    # Query Beacon initially\n",
    "    before = queryBeacon(curBeacon)\n",
    "    # Add people\n",
    "    updatedBeacon = np.concatenate([added_people, curBeacon])\n",
    "    # Query Beacon again\n",
    "    after = queryBeacon(updatedBeacon)\n",
    "    # Find No-Yes SNPs' indices\n",
    "    no_yes_indices = np.where(np.logical_and(before == False, after == True))[0]\n",
    "    yes_yes_indices = np.where(np.logical_and(before == True, after == True))[0]\n",
    "    print(\"Number of No-Yes SNP's : \", len(no_yes_indices))\n",
    "\n",
    "    return yes_yes_indices, no_yes_indices, added_people, curBeacon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: Correlation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def builtSNPNetwork(no_yes_indices, model_ind, reference):\n",
    "    model = ternary[no_yes_ind][:, model_ind].astype(float)\n",
    "    model[model==-1] = np.nan\n",
    "    x = pairwise_distances(model, metric = \"sokalmichener\", n_jobs=-1)\n",
    "    x = 1-np.nan_to_num(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_method(no_yes_indices, add_count, cluster_count=None):\n",
    "    c = maf.iloc[no_yes_indices]\n",
    "\n",
    "    # Calculate probabilities of SNP possibilities\n",
    "    greater = c.loc[c['major_freq'] >= c['minor_freq']]\n",
    "    smaller = c.loc[c['major_freq'] < c['minor_freq']]\n",
    "\n",
    "    greater[\"maj-maj\"] = greater['major'] + \"\" + greater['major']\n",
    "    greater[\"mean\"] = pd.concat([greater['major'] + \"\" + greater['minor'], greater['minor'] + \"\" + greater['major']], axis=1).min(axis=1)\n",
    "    greater[\"min-min\"] = greater['minor'] + \"\" + greater['minor']\n",
    "    greater[\"p1\"] = greater['major_freq']**2\n",
    "    greater[\"p2\"] = 2*greater['major_freq']*greater['minor_freq']\n",
    "    greater[\"p3\"] = greater['minor_freq']**2\n",
    "\n",
    "    smaller[\"maj-maj\"] = smaller['minor'] + \"\" + smaller['minor']\n",
    "    smaller[\"mean\"] = pd.concat([smaller['major'] + \"\" + smaller['minor'], smaller['minor'] + \"\" + smaller['major']], axis=1).min(axis=1)\n",
    "    smaller[\"min-min\"] = smaller['major'] + \"\" + smaller['major']\n",
    "    smaller[\"p1\"] = smaller['minor_freq']**2\n",
    "    smaller[\"p2\"] = 2*smaller['major_freq']*smaller['minor_freq']\n",
    "    smaller[\"p3\"] = smaller['major_freq']**2\n",
    "\n",
    "    tt = pd.concat([greater,smaller], axis=0)\n",
    "    tt.sort_index(inplace=True)\n",
    "\n",
    "    genome_possibilities = tt[[\"maj-maj\", \"mean\", \"min-min\"]].values\n",
    "    probabilities = tt[[\"p1\",\"p2\",\"p3\"]].values\n",
    "\n",
    "    mutations = tt[[\"mean\", \"min-min\"]].values\n",
    "    mutation_probs = tt[[\"p2\",\"p3\"]].values\n",
    "\n",
    "    # Randomly reconstruct the people's genome\n",
    "    bins = []\n",
    "    cumulative = probabilities.cumsum(axis=1)\n",
    "    for i in range(add_count):\n",
    "        uniform = np.random.rand(len(cumulative), 1)\n",
    "        choices = (uniform < cumulative).argmax(axis=1)\n",
    "        reconstructed = np.choose(choices, genome_possibilities.T)\n",
    "        bins.append(reconstructed)\n",
    "    bins = np.array(bins)\n",
    "    \n",
    "    # Be sure that at least one person has the mutation\n",
    "    equality = np.sum((bins == reference[no_yes_indices].T), axis=0)\n",
    "    changed_indices = np.where(equality==add_count)[0]\n",
    "\n",
    "    index_choices = np.random.randint(add_count, size=len(equality))[changed_indices]\n",
    "\n",
    "    non_zeros = mutation_probs[np.sum(mutation_probs, axis=1) != 0]\n",
    "    probs = (non_zeros.T / np.sum(non_zeros, axis=1).T).T\n",
    "\n",
    "    zeros = np.zeros((mutation_probs.shape[0], 2))\n",
    "    zeros[np.sum(mutation_probs, axis=1) != 0] = probs\n",
    "    probs = zeros[changed_indices]\n",
    "\n",
    "    cum = probs.cumsum(axis=1)\n",
    "    uni = np.random.rand(len(cum), 1)\n",
    "    choi = (uni < cum).argmax(axis=1)\n",
    "    res = np.choose(choi, mutations[changed_indices].T)\n",
    "\n",
    "    bins.T[changed_indices, index_choices] = res\n",
    "    # Reconstruct\n",
    "    reconstructed = np.array([reference.T[0] for i in range(add_count)])\n",
    "    reconstructed.T[no_yes_indices] = bins.T\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectralClustering(no_yes_indices, add_count, x, reference, cluster_count=None):\n",
    "    if not cluster_count:\n",
    "        cluster_count = add_count\n",
    "    sc = SpectralClustering(cluster_count, affinity='precomputed', n_init=100, n_jobs=-1)\n",
    "    sc.fit(np.array(x))\n",
    "    bins = []\n",
    "    for i in range(cluster_count):\n",
    "        temp = []\n",
    "        for element in np.where(sc.labels_==i)[0]:\n",
    "            temp.append(no_yes_indices[element])\n",
    "        #print(\"Bin \" + str(i) + \" has \" + str(len(temp)) + \" SNP's\")\n",
    "        bins.append(temp)\n",
    "    reconstructed = np.array([reference.T[0] for i in range(cluster_count)])\n",
    "    for i in range(cluster_count):\n",
    "        for j in bins[i]:\n",
    "            reconstructed[i][j] = getMutationAt(j)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuzzy Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzyClustering(no_yes_indices, add_count, x, reference, cluster_count=None):\n",
    "    if not cluster_count:\n",
    "        cluster_count = add_count\n",
    "    fcm = FCM(n_clusters=cluster_count)\n",
    "    fcm.fit(correlations)\n",
    "    soft_clusters = fcm.u\n",
    "    bins = [[] for i in range(cluster_count)]\n",
    "    for i in range(len(soft_clusters)):\n",
    "        maxPos = np.argmax(soft_clusters[i])\n",
    "        if soft_clusters[i][maxPos] <= 0.5:\n",
    "            for j in np.where(soft_clusters[i] > (soft_clusters[i][maxPos] * 2 / 3))[0]:\n",
    "                bins[j].append(no_yes_indices[i])\n",
    "        else:\n",
    "            bins[maxPos].append(no_yes_indices[i])\n",
    "    reconstructed = np.array([reference.T[0] for i in range(cluster_count)])\n",
    "    for i in range(cluster_count):\n",
    "        for j in bins[i]:\n",
    "            reconstructed[i][j] = getMutationAt(j)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E X P E R I M E N T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of No-Yes SNP's :  14783\n"
     ]
    }
   ],
   "source": [
    "add_count = 5\n",
    "cluster_count = 5\n",
    "beacon_size = 50\n",
    "yes_yes_ind, no_yes_ind, added_people, beacon_people     = getNoYes2(add_count, beacon_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall_Micro_Avg    = 0.75 \n",
      "Precision_Micro_Avg = 0.72\n"
     ]
    }
   ],
   "source": [
    "model_ind = allPeople\n",
    "correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "(precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Added People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall_Micro_Avg    = 0.55 \n",
      "Precision_Micro_Avg = 0.42\n"
     ]
    }
   ],
   "source": [
    "model_ind = np.setdiff1d(allPeople, added_people)\n",
    "model_ind = np.setdiff1d(model_ind, beacon_people)\n",
    "\n",
    "correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "(precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_count = 2\n",
    "cluster_count = 2\n",
    "beacon_size = 50\n",
    "yes_yes_ind, no_yes_ind, added_people     = getNoYes(add_count, beacon_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ind = np.setdiff1d(otherPeople, added_people)\n",
    "correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "(precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,cluster_count,no_yes_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Tests and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [(2,20),(3,30),(4,40),(5,50),(10,100),(20,100)]\n",
    "res = []\n",
    "for e in experiments:\n",
    "    add_count = e[0]\n",
    "    beacon_size = e[1]\n",
    "\n",
    "    for i in range(20):\n",
    "        yes_yes_ind, no_yes_ind, added_people = getNoYes(add_count, beacon_size)\n",
    "        model_ind = np.setdiff1d(otherPeople, added_people)\n",
    "\n",
    "        # Genome Reconstruction    \n",
    "        correlations                              = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "        reconstructed_spectral                    = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "        (precision,recall,accuracy), _, matches   = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,add_count,no_yes_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Vary Added People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = [2,3,5,10,20]\n",
    "beacon_size = 50\n",
    "run_count = 20\n",
    "\n",
    "results = np.zeros((3, len(counts), run_count, 3))\n",
    "\n",
    "for i in range(len(counts)):\n",
    "    for j in range(run_count):\n",
    "        yes_yes_ind, no_yes_ind, added_people = getNoYes(counts[i], beacon_size)\n",
    "        model_ind = np.setdiff1d(otherPeople, added_people)\n",
    "\n",
    "        # Genome Reconstruction    \n",
    "        correlations                = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "        reconstructed_spectral      = spectralClustering(no_yes_ind, counts[i], correlations, reference)\n",
    "        results[0, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,counts[i],counts[i],no_yes_ind) \n",
    "\n",
    "        # Baseline\n",
    "        reconstructed_baseline      = baseline_method(no_yes_ind, counts[i])\n",
    "        results[1, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T, reconstructed_baseline,counts[i],counts[i],no_yes_ind)  \n",
    "\n",
    "        # Fuzzy\n",
    "        reconstructed_fuzzy         = fuzzyClustering(no_yes_ind, counts[i], correlations, reference)\n",
    "        results[2, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T, reconstructed_fuzzy, counts[i], counts[i], no_yes_ind)  \n",
    "        \n",
    "with open(\"../results/1C-Triple-VaryAdded.pickle\", 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Vary Beacon Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [25,50,75,100]\n",
    "add_count = 5\n",
    "run_count = 20\n",
    "\n",
    "results = np.zeros((3, len(counts), run_count, 3))\n",
    "\n",
    "for i in range(len(counts)):\n",
    "    for j in range(run_count):\n",
    "        yes_yes_ind, no_yes_ind, added_people = getNoYes(add_count, counts[i])\n",
    "        model_ind = np.setdiff1d(otherPeople, added_people)\n",
    "\n",
    "        # Genome Reconstruction    \n",
    "        correlations                = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "        reconstructed_spectral      = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "        results[0, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,add_count,no_yes_ind) \n",
    "\n",
    "        # Baseline\n",
    "        reconstructed_baseline      = baseline_method(no_yes_ind, add_count)\n",
    "        results[1, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T, reconstructed_baseline,add_count,add_count,no_yes_ind)  \n",
    "\n",
    "        # Fuzzy\n",
    "        reconstructed_fuzzy         = fuzzyClustering(no_yes_ind,add_count,correlations,reference)\n",
    "        results[2, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T, reconstructed_fuzzy,add_count,add_count,no_yes_ind)  \n",
    "        \n",
    "with open(\"../results/1C-Triple-VaryBeacon.pickle\", 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Cluster Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [1,2,3,4,5,10]\n",
    "add_count = 5\n",
    "run_count = 20\n",
    "beacon_size = 50\n",
    "\n",
    "results = np.zeros((3, len(counts), run_count, 3))\n",
    "\n",
    "for i in range(len(counts)):\n",
    "    for j in range(run_count):\n",
    "        yes_yes_ind, no_yes_ind, added_people = getNoYes(add_count, beacon_size)\n",
    "        model_ind = np.setdiff1d(otherPeople, added_people)\n",
    "\n",
    "        # Genome Reconstruction    \n",
    "        correlations                = builtSNPNetwork(no_yes_ind, model_ind, reference)\n",
    "        reconstructed_spectral      = spectralClustering(no_yes_ind, add_count, correlations, reference)\n",
    "        results[0, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T,reconstructed_spectral,add_count,counts[i],no_yes_ind) \n",
    "\n",
    "        # Baseline\n",
    "        reconstructed_baseline      = baseline_method(no_yes_ind, add_count)\n",
    "        results[1, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T, reconstructed_baseline,add_count,counts[i],no_yes_ind)  \n",
    "\n",
    "        # Fuzzy\n",
    "        reconstructed_fuzzy         = fuzzyClustering(no_yes_ind,add_count,correlations,reference)\n",
    "        results[2, i, j, :],_,_     = performance_f(beacon.iloc[:, added_people].values.T, reconstructed_fuzzy,add_count,counts[i],no_yes_ind)  \n",
    "        \n",
    "with open(\"../results/1C-Triple-VaryCluster.pickle\", 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
